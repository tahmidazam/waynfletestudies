<!DOCTYPE html> <html><head>
		<title>Prospects of brain-computer interfaces in communication for disabled children</title>
		<base href="./">
		<meta id="root-path" root-path="./">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes, minimum-scale=1.0, maximum-scale=5.0">
		<meta charset="UTF-8">
		<meta name="description" content="Tripos - Prospects of brain-computer interfaces in communication for disabled children">
		<meta property="og:title" content="Prospects of brain-computer interfaces in communication for disabled children">
		<meta property="og:description" content="Tripos - Prospects of brain-computer interfaces in communication for disabled children">
		<meta property="og:type" content="website">
		<meta property="og:url" content="prospects-of-brain-computer-interfaces-in-communication-for-disabled-children.html">
		<meta property="og:image" content="figures/figure-1.jpg">
		<meta property="og:site_name" content="Tripos">
		<link rel="alternate" type="application/rss+xml" title="RSS Feed" href="lib/rss.xml"><script async="" id="webpage-script" src="lib/scripts/webpage.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><link rel="icon" href="lib/media/favicon.png"><style>body{--line-width:40em;--line-width-adaptive:40em;--file-line-width:40em;--sidebar-width:min(20em, 80vw);--collapse-arrow-size:11px;--tree-horizontal-spacing:0.6em;--tree-vertical-spacing:0.6em;--sidebar-margin:12px}.sidebar{height:100%;min-width:calc(var(--sidebar-width) + var(--divider-width-hover));max-width:calc(var(--sidebar-width) + var(--divider-width-hover));font-size:14px;z-index:10;position:relative;overflow:hidden;transition:min-width ease-in-out,max-width ease-in-out;transition-duration:.2s;contain:size}.sidebar-left{left:0}.sidebar-right{right:0}.sidebar.is-collapsed{min-width:0;max-width:0}body.floating-sidebars .sidebar{position:absolute}.sidebar-content{height:100%;min-width:calc(var(--sidebar-width) - var(--divider-width-hover));top:0;padding:var(--sidebar-margin);padding-top:4em;line-height:var(--line-height-tight);background-color:var(--background-secondary);transition:background-color,border-right,border-left,box-shadow;transition-duration:var(--color-fade-speed);transition-timing-function:ease-in-out;position:absolute;display:flex;flex-direction:column}.sidebar:not(.is-collapsed) .sidebar-content{min-width:calc(max(100%,var(--sidebar-width)) - 3px);max-width:calc(max(100%,var(--sidebar-width)) - 3px)}.sidebar-left .sidebar-content{left:0;border-top-right-radius:var(--radius-l);border-bottom-right-radius:var(--radius-l)}.sidebar-right .sidebar-content{right:0;border-top-left-radius:var(--radius-l);border-bottom-left-radius:var(--radius-l)}.sidebar:has(.sidebar-content:empty):has(.topbar-content:empty){display:none}.sidebar-topbar{height:2em;width:var(--sidebar-width);top:var(--sidebar-margin);padding-inline:var(--sidebar-margin);z-index:1;position:fixed;display:flex;align-items:center;transition:width ease-in-out;transition-duration:inherit}.sidebar.is-collapsed .sidebar-topbar{width:calc(2.3em + var(--sidebar-margin) * 2)}.sidebar .sidebar-topbar.is-collapsed{width:0}.sidebar-left .sidebar-topbar{left:0}.sidebar-right .sidebar-topbar{right:0}.topbar-content{overflow:hidden;overflow:clip;width:100%;height:100%;display:flex;align-items:center;transition:inherit}.sidebar.is-collapsed .topbar-content{width:0;transition:inherit}.clickable-icon.sidebar-collapse-icon{background-color:transparent;color:var(--icon-color-focused);padding:0!important;margin:0!important;height:100%!important;width:2.3em!important;margin-inline:0.14em!important;position:absolute}.sidebar-left .clickable-icon.sidebar-collapse-icon{transform:rotateY(180deg);right:var(--sidebar-margin)}.sidebar-right .clickable-icon.sidebar-collapse-icon{transform:rotateY(180deg);left:var(--sidebar-margin)}.clickable-icon.sidebar-collapse-icon svg.svg-icon{width:100%;height:100%}.sidebar-section-header{margin:0 0 1em 0;text-transform:uppercase;letter-spacing:.06em;font-weight:600}body{transition:background-color var(--color-fade-speed) ease-in-out}.webpage-container{display:flex;flex-direction:row;height:100%;width:100%;align-items:stretch;justify-content:center}.document-container{opacity:1;flex-basis:100%;max-width:100%;width:100%;height:100%;display:flex;flex-direction:column;align-items:center;transition:opacity .2s ease-in-out;contain:inline-size}.hide{opacity:0;transition:opacity .2s ease-in-out}.document-container>.markdown-preview-view{margin:var(--sidebar-margin);margin-bottom:0;width:100%;width:-webkit-fill-available;width:-moz-available;width:fill-available;background-color:var(--background-primary);transition:background-color var(--color-fade-speed) ease-in-out;border-top-right-radius:var(--window-radius,var(--radius-m));border-top-left-radius:var(--window-radius,var(--radius-m));overflow-x:hidden!important;overflow-y:auto!important;display:flex!important;flex-direction:column!important;align-items:center!important;contain:inline-size}.document-container>.markdown-preview-view>.markdown-preview-sizer{padding-bottom:80vh!important;width:100%!important;max-width:var(--line-width)!important;flex-basis:var(--line-width)!important;transition:background-color var(--color-fade-speed) ease-in-out;contain:inline-size}.markdown-rendered img:not([width]),.view-content img:not([width]){max-width:100%;outline:0}.document-container>.view-content.embed{display:flex;padding:1em;height:100%;width:100%;align-items:center;justify-content:center}.document-container>.view-content.embed>*{max-width:100%;max-height:100%;object-fit:contain}:has(> :is(.math,table)){overflow-x:auto!important}.document-container>.view-content{overflow-x:auto;contain:content;padding:0;margin:0;height:100%}.scroll-highlight{position:absolute;width:100%;height:100%;pointer-events:none;z-index:1000;background-color:hsla(var(--color-accent-hsl),.25);opacity:0;padding:1em;inset:50%;translate:-50% -50%;border-radius:var(--radius-s)}</style><script defer="">async function loadIncludes(){if("file:"!=location.protocol){let e=document.querySelectorAll("include");for(let t=0;t<e.length;t++){let o=e[t],l=o.getAttribute("src");try{const e=await fetch(l);if(!e.ok){console.log("Could not include file: "+l),o?.remove();continue}let t=await e.text(),n=document.createRange().createContextualFragment(t),i=Array.from(n.children);for(let e of i)e.classList.add("hide"),e.style.transition="opacity 0.5s ease-in-out",setTimeout((()=>{e.classList.remove("hide")}),10);o.before(n),o.remove(),console.log("Included file: "+l)}catch(e){o?.remove(),console.log("Could not include file: "+l,e);continue}}}else{if(document.querySelectorAll("include").length>0){var e=document.createElement("div");e.id="error",e.textContent="Web server exports must be hosted on an http / web server to be viewed correctly.",e.style.position="fixed",e.style.top="50%",e.style.left="50%",e.style.transform="translate(-50%, -50%)",e.style.fontSize="1.5em",e.style.fontWeight="bold",e.style.textAlign="center",document.body.appendChild(e),document.querySelector(".document-container")?.classList.remove("hide")}}}document.addEventListener("DOMContentLoaded",(()=>{loadIncludes()}));let isFileProtocol="file:"==location.protocol;function waitLoadScripts(e,t){let o=e.map((e=>document.getElementById(e+"-script"))),l=0;!function e(){let n=o[l];l++,n&&"true"!=n.getAttribute("loaded")||l<o.length&&e(),l<o.length?n.addEventListener("load",e):t()}()}</script><link rel="stylesheet" href="lib/styles/obsidian.css"><link rel="preload" href="lib/styles/global-variable-styles.css" as="style" onload="this.onload=null;this.rel='stylesheet'"><noscript><link rel="stylesheet" href="lib/styles/global-variable-styles.css"></noscript><link rel="preload" href="lib/styles/main-styles.css" as="style" onload="this.onload=null;this.rel='stylesheet'"><noscript><link rel="stylesheet" href="lib/styles/main-styles.css"></noscript><link rel="preload" href="lib/styles/snippets.css" as="style" onload="this.onload=null;this.rel='stylesheet'"><noscript><link rel="stylesheet" href="lib/styles/snippets.css"></noscript></head><body class="publish css-settings-manager native-scrollbars theme-light show-inline-title"><script defer="">let theme=localStorage.getItem("theme")||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light");"dark"==theme?(document.body.classList.add("theme-dark"),document.body.classList.remove("theme-light")):(document.body.classList.add("theme-light"),document.body.classList.remove("theme-dark")),window.innerWidth<480?document.body.classList.add("is-phone"):window.innerWidth<768?document.body.classList.add("is-tablet"):window.innerWidth<1024?document.body.classList.add("is-small-screen"):document.body.classList.add("is-large-screen")</script><div class="webpage-container workspace"><div class="sidebar-left sidebar"><div class="sidebar-handle"></div><div class="sidebar-topbar"><div class="topbar-content"></div><div class="clickable-icon sidebar-collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="100%" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="3" stroke-linecap="round" stroke-linejoin="round" class="svg-icon"><path d="M21 3H3C1.89543 3 1 3.89543 1 5V19C1 20.1046 1.89543 21 3 21H21C22.1046 21 23 20.1046 23 19V5C23 3.89543 22.1046 3 21 3Z"></path><path d="M10 4V20"></path><path d="M4 7H7"></path><path d="M4 10H7"></path><path d="M4 13H7"></path></svg></div></div><div class="sidebar-content"></div><script defer="">let ls = document.querySelector(".sidebar-left"); ls.classList.add("is-collapsed"); if (window.innerWidth > 768) ls.classList.remove("is-collapsed"); ls.style.setProperty("--sidebar-width", localStorage.getItem("sidebar-left-width"));</script></div><div class="document-container markdown-reading-view hide"><div class="markdown-preview-view markdown-rendered allow-fold-headings allow-fold-lists is-readable-line-width"><style id="MJX-CHTML-styles">mjx-c.mjx-c2211.TEX-S1::before { padding: 0.75em 1.056em 0.25em 0px; content: "∑"; }
mjx-c.mjx-c75::before { padding: 0.442em 0.556em 0.011em 0px; content: "u"; }
mjx-c.mjx-c73::before { padding: 0.448em 0.394em 0.011em 0px; content: "s"; }
mjx-c.mjx-c1D452.TEX-I::before { padding: 0.442em 0.466em 0.011em 0px; content: "e"; }
mjx-mfrac { display: inline-block; text-align: left; }
mjx-frac { display: inline-block; vertical-align: 0.17em; padding: 0px 0.22em; }
mjx-frac[type="d"] { vertical-align: 0.04em; }
mjx-frac[delims] { padding: 0px 0.1em; }
mjx-frac[atop] { padding: 0px 0.12em; }
mjx-frac[atop][delims] { padding: 0px; }
mjx-dtable { display: inline-table; width: 100%; }
mjx-dtable > * { font-size: 2000%; }
mjx-dbox { display: block; font-size: 5%; }
mjx-num { display: block; text-align: center; }
mjx-den { display: block; text-align: center; }
mjx-mfrac[bevelled] > mjx-num { display: inline-block; }
mjx-mfrac[bevelled] > mjx-den { display: inline-block; }
mjx-den[align="right"], mjx-num[align="right"] { text-align: right; }
mjx-den[align="left"], mjx-num[align="left"] { text-align: left; }
mjx-nstrut { display: inline-block; height: 0.054em; width: 0px; vertical-align: -0.054em; }
mjx-nstrut[type="d"] { height: 0.217em; vertical-align: -0.217em; }
mjx-dstrut { display: inline-block; height: 0.505em; width: 0px; }
mjx-dstrut[type="d"] { height: 0.726em; }
mjx-line { display: block; box-sizing: border-box; min-height: 1px; height: 0.06em; border-top: 0.06em solid; margin: 0.06em -0.1em; overflow: hidden; }
mjx-line[type="d"] { margin: 0.18em -0.1em; }
mjx-c.mjx-c61::before { padding: 0.448em 0.5em 0.011em 0px; content: "a"; }
mjx-mtext { display: inline-block; text-align: left; }
mjx-c.mjx-c5C::before { padding: 0.75em 0.5em 0.25em 0px; content: "\\"; }
mjx-c.mjx-c66::before { padding: 0.705em 0.372em 0px 0px; content: "f"; }
mjx-c.mjx-c72::before { padding: 0.442em 0.392em 0px 0px; content: "r"; }
mjx-c.mjx-c29::before { padding: 0.75em 0.389em 0.25em 0px; content: ")"; }
mjx-c.mjx-c28::before { padding: 0.75em 0.389em 0.25em 0px; content: "("; }
mjx-c.mjx-c1D44E.TEX-I::before { padding: 0.441em 0.529em 0.01em 0px; content: "a"; }
mjx-c.mjx-c1D446.TEX-I::before { padding: 0.705em 0.645em 0.022em 0px; content: "S"; }
mjx-mover { display: inline-block; text-align: left; }
mjx-mover:not([limits="false"]) { padding-top: 0.1em; }
mjx-mover:not([limits="false"]) > * { display: block; text-align: left; }
mjx-mn { display: inline-block; text-align: left; }
mjx-c.mjx-c394::before { padding: 0.716em 0.833em 0px 0px; content: "Δ"; }
mjx-c.mjx-cAF::before { padding: 0.59em 0.5em 0px 0px; content: "¯"; }
mjx-c.mjx-c1D443.TEX-I::before { padding: 0.683em 0.751em 0px 0px; content: "P"; }
mjx-c.mjx-c31::before { padding: 0.666em 0.5em 0px 0px; content: "1"; }
mjx-c.mjx-c3A::before { padding: 0.43em 0.278em 0px 0px; content: ":"; }
mjx-c.mjx-c32::before { padding: 0.666em 0.5em 0px 0px; content: "2"; }
mjx-c.mjx-c2248::before { padding: 0.483em 0.778em 0px 0px; content: "≈"; }
mjx-c.mjx-c30::before { padding: 0.666em 0.5em 0.022em 0px; content: "0"; }
mjx-c.mjx-c1D707.TEX-I::before { padding: 0.442em 0.603em 0.216em 0px; content: "μ"; }
mjx-c.mjx-c1D6FD.TEX-I::before { padding: 0.705em 0.566em 0.194em 0px; content: "β"; }
mjx-c.mjx-c5B.TEX-S4::before { padding: 1.75em 0.583em 1.249em 0px; content: "["; }
mjx-c.mjx-c5D.TEX-S4::before { padding: 1.75em 0.583em 1.249em 0px; content: "]"; }
mjx-mo { display: inline-block; text-align: left; }
mjx-stretchy-h { display: inline-table; width: 100%; }
mjx-stretchy-h > * { display: table-cell; width: 0px; }
mjx-stretchy-h > * > mjx-c { display: inline-block; transform: scaleX(1); }
mjx-stretchy-h > * > mjx-c::before { display: inline-block; width: initial; }
mjx-stretchy-h > mjx-ext { overflow: clip visible; width: 100%; }
mjx-stretchy-h > mjx-ext > mjx-c::before { transform: scaleX(500); }
mjx-stretchy-h > mjx-ext > mjx-c { width: 0px; }
mjx-stretchy-h > mjx-beg > mjx-c { margin-right: -0.1em; }
mjx-stretchy-h > mjx-end > mjx-c { margin-left: -0.1em; }
mjx-stretchy-v { display: inline-block; }
mjx-stretchy-v > * { display: block; }
mjx-stretchy-v > mjx-beg { height: 0px; }
mjx-stretchy-v > mjx-end > mjx-c { display: block; }
mjx-stretchy-v > * > mjx-c { transform: scaleY(1); transform-origin: left center; overflow: hidden; }
mjx-stretchy-v > mjx-ext { display: block; height: 100%; box-sizing: border-box; border: 0px solid transparent; overflow: visible clip; }
mjx-stretchy-v > mjx-ext > mjx-c::before { width: initial; box-sizing: border-box; }
mjx-stretchy-v > mjx-ext > mjx-c { transform: scaleY(500) translateY(0.075em); overflow: visible; }
mjx-mark { display: inline-block; height: 0px; }
mjx-mrow { display: inline-block; text-align: left; }
mjx-munder { display: inline-block; text-align: left; }
mjx-over { text-align: left; }
mjx-munder:not([limits="false"]) { display: inline-table; }
mjx-munder > mjx-row { text-align: left; }
mjx-under { padding-bottom: 0.1em; }
mjx-stretchy-v.mjx-c5B mjx-beg mjx-c::before { content: "⎡"; padding: 1.154em 0.667em 0.645em 0px; }
mjx-stretchy-v.mjx-c5B mjx-ext mjx-c::before { content: "⎢"; width: 0.667em; }
mjx-stretchy-v.mjx-c5B mjx-end mjx-c::before { content: "⎣"; padding: 1.155em 0.667em 0.644em 0px; }
mjx-stretchy-v.mjx-c5B > mjx-end { margin-top: -1.799em; }
mjx-stretchy-v.mjx-c5B > mjx-ext { border-top-width: 1.769em; border-bottom-width: 1.769em; }
mjx-stretchy-v.mjx-c5D mjx-beg mjx-c::before { content: "⎤"; padding: 1.154em 0.667em 0.645em 0px; }
mjx-stretchy-v.mjx-c5D mjx-ext mjx-c::before { content: "⎥"; width: 0.667em; }
mjx-stretchy-v.mjx-c5D mjx-end mjx-c::before { content: "⎦"; padding: 1.155em 0.667em 0.644em 0px; }
mjx-stretchy-v.mjx-c5D > mjx-end { margin-top: -1.799em; }
mjx-stretchy-v.mjx-c5D > mjx-ext { border-top-width: 1.769em; border-bottom-width: 1.769em; }
mjx-c.mjx-c1D459.TEX-I::before { padding: 0.694em 0.298em 0.011em 0px; content: "l"; }
mjx-c.mjx-c2212::before { padding: 0.583em 0.778em 0.082em 0px; content: "−"; }
mjx-c.mjx-c3D::before { padding: 0.583em 0.778em 0.082em 0px; content: "="; }
mjx-c.mjx-c2211.TEX-S2::before { padding: 0.95em 1.444em 0.45em 0px; content: "∑"; }
mjx-c.mjx-c5B::before { padding: 0.75em 0.278em 0.25em 0px; content: "["; }
mjx-c.mjx-c5D::before { padding: 0.75em 0.278em 0.25em 0px; content: "]"; }
mjx-c.mjx-cD7::before { padding: 0.491em 0.778em 0px 0px; content: "×"; }
mjx-container[jax="CHTML"] { line-height: 0; }
mjx-container [space="1"] { margin-left: 0.111em; }
mjx-container [space="2"] { margin-left: 0.167em; }
mjx-container [space="3"] { margin-left: 0.222em; }
mjx-container [space="4"] { margin-left: 0.278em; }
mjx-container [space="5"] { margin-left: 0.333em; }
mjx-container [rspace="1"] { margin-right: 0.111em; }
mjx-container [rspace="2"] { margin-right: 0.167em; }
mjx-container [rspace="3"] { margin-right: 0.222em; }
mjx-container [rspace="4"] { margin-right: 0.278em; }
mjx-container [rspace="5"] { margin-right: 0.333em; }
mjx-container [size="s"] { font-size: 70.7%; }
mjx-container [size="ss"] { font-size: 50%; }
mjx-container [size="Tn"] { font-size: 60%; }
mjx-container [size="sm"] { font-size: 85%; }
mjx-container [size="lg"] { font-size: 120%; }
mjx-container [size="Lg"] { font-size: 144%; }
mjx-container [size="LG"] { font-size: 173%; }
mjx-container [size="hg"] { font-size: 207%; }
mjx-container [size="HG"] { font-size: 249%; }
mjx-container [width="full"] { width: 100%; }
mjx-box { display: inline-block; }
mjx-block { display: block; }
mjx-itable { display: inline-table; }
mjx-row { display: table-row; }
mjx-row > * { display: table-cell; }
mjx-mtext { display: inline-block; }
mjx-mstyle { display: inline-block; }
mjx-merror { display: inline-block; color: red; background-color: yellow; }
mjx-mphantom { visibility: hidden; }
mjx-assistive-mml { top: 0px; left: 0px; clip: rect(1px, 1px, 1px, 1px); user-select: none; position: absolute !important; padding: 1px 0px 0px !important; border: 0px !important; display: block !important; width: auto !important; overflow: hidden !important; }
mjx-assistive-mml[display="block"] { width: 100% !important; }
mjx-math { display: inline-block; text-align: left; line-height: 0; text-indent: 0px; font-style: normal; font-weight: normal; font-size: 100%; letter-spacing: normal; border-collapse: collapse; overflow-wrap: normal; word-spacing: normal; white-space: nowrap; direction: ltr; padding: 1px 0px; }
mjx-container[jax="CHTML"][display="true"] { display: block; text-align: center; margin: 1em 0px; }
mjx-container[jax="CHTML"][display="true"][width="full"] { display: flex; }
mjx-container[jax="CHTML"][display="true"] mjx-math { padding: 0px; }
mjx-container[jax="CHTML"][justify="left"] { text-align: left; }
mjx-container[jax="CHTML"][justify="right"] { text-align: right; }
mjx-msub { display: inline-block; text-align: left; }
mjx-mi { display: inline-block; text-align: left; }
mjx-c { display: inline-block; }
mjx-utext { display: inline-block; padding: 0.75em 0px 0.2em; }
mjx-texatom { display: inline-block; text-align: left; }
mjx-msup { display: inline-block; text-align: left; }
mjx-c::before { display: block; width: 0px; }
.MJX-TEX { font-family: MJXZERO, MJXTEX; }
.TEX-B { font-family: MJXZERO, MJXTEX-B; }
.TEX-I { font-family: MJXZERO, MJXTEX-I; }
.TEX-MI { font-family: MJXZERO, MJXTEX-MI; }
.TEX-BI { font-family: MJXZERO, MJXTEX-BI; }
.TEX-S1 { font-family: MJXZERO, MJXTEX-S1; }
.TEX-S2 { font-family: MJXZERO, MJXTEX-S2; }
.TEX-S3 { font-family: MJXZERO, MJXTEX-S3; }
.TEX-S4 { font-family: MJXZERO, MJXTEX-S4; }
.TEX-A { font-family: MJXZERO, MJXTEX-A; }
.TEX-C { font-family: MJXZERO, MJXTEX-C; }
.TEX-CB { font-family: MJXZERO, MJXTEX-CB; }
.TEX-FR { font-family: MJXZERO, MJXTEX-FR; }
.TEX-FRB { font-family: MJXZERO, MJXTEX-FRB; }
.TEX-SS { font-family: MJXZERO, MJXTEX-SS; }
.TEX-SSB { font-family: MJXZERO, MJXTEX-SSB; }
.TEX-SSI { font-family: MJXZERO, MJXTEX-SSI; }
.TEX-SC { font-family: MJXZERO, MJXTEX-SC; }
.TEX-T { font-family: MJXZERO, MJXTEX-T; }
.TEX-V { font-family: MJXZERO, MJXTEX-V; }
.TEX-VB { font-family: MJXZERO, MJXTEX-VB; }
mjx-stretchy-v mjx-c, mjx-stretchy-h mjx-c { font-family: MJXZERO, MJXTEX-S1, MJXTEX-S4, MJXTEX, MJXTEX-A !important; }
@font-face { font-family: MJXZERO; src: url("lib/fonts/mathjax_zero.woff") format("woff"); }
@font-face { font-family: MJXTEX; src: url("lib/fonts/mathjax_main-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-B; src: url("lib/fonts/mathjax_main-bold.woff") format("woff"); }
@font-face { font-family: MJXTEX-I; src: url("lib/fonts/mathjax_math-italic.woff") format("woff"); }
@font-face { font-family: MJXTEX-MI; src: url("lib/fonts/mathjax_main-italic.woff") format("woff"); }
@font-face { font-family: MJXTEX-BI; src: url("lib/fonts/mathjax_math-bolditalic.woff") format("woff"); }
@font-face { font-family: MJXTEX-S1; src: url("lib/fonts/mathjax_size1-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-S2; src: url("lib/fonts/mathjax_size2-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-S3; src: url("lib/fonts/mathjax_size3-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-S4; src: url("lib/fonts/mathjax_size4-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-A; src: url("lib/fonts/mathjax_ams-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-C; src: url("lib/fonts/mathjax_calligraphic-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-CB; src: url("lib/fonts/mathjax_calligraphic-bold.woff") format("woff"); }
@font-face { font-family: MJXTEX-FR; src: url("lib/fonts/mathjax_fraktur-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-FRB; src: url("lib/fonts/mathjax_fraktur-bold.woff") format("woff"); }
@font-face { font-family: MJXTEX-SS; src: url("lib/fonts/mathjax_sansserif-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-SSB; src: url("lib/fonts/mathjax_sansserif-bold.woff") format("woff"); }
@font-face { font-family: MJXTEX-SSI; src: url("lib/fonts/mathjax_sansserif-italic.woff") format("woff"); }
@font-face { font-family: MJXTEX-SC; src: url("lib/fonts/mathjax_script-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-T; src: url("lib/fonts/mathjax_typewriter-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-V; src: url("lib/fonts/mathjax_vector-regular.woff") format("woff"); }
@font-face { font-family: MJXTEX-VB; src: url("lib/fonts/mathjax_vector-bold.woff") format("woff"); }
mjx-c.mjx-c1D463.TEX-I::before { padding: 0.443em 0.485em 0.011em 0px; content: "v"; }
mjx-c.mjx-c1D457.TEX-I::before { padding: 0.661em 0.412em 0.204em 0px; content: "j"; }
mjx-c.mjx-c1D464.TEX-I::before { padding: 0.443em 0.716em 0.011em 0px; content: "w"; }
mjx-c.mjx-c2032::before { padding: 0.56em 0.275em 0px 0px; content: "′"; }
</style><div class="markdown-preview-sizer markdown-preview-section"><h1 class="page-title heading inline-title" id="Prospects of brain-computer interfaces in communication for disabled children">Prospects of brain-computer interfaces in communication for disabled children</h1><div><p>Tahmid Azam (<a data-tooltip-position="top" aria-label="mailto:ta549@cam.ac.uk" rel="noopener" class="external-link" href="mailto:ta549@cam.ac.uk" target="_blank">ta549@cam.ac.uk</a>), May 2022</p></div><div><p>All figures, except for Figure 1, Figure 3, Figure 4, Figure 5 and Figure 6 were either programmatically generated using MATLAB and EEGLAB, created in Adobe Photoshop, or plotted in Microsoft Excel for this study. All scripts and functions for both the generation of figures and the research are original and were written for this study. The scripts, functions, interview transcript, figures and the processed dataset can be found in the study’s  <a data-tooltip-position="top" aria-label="https://github.com/tahmidazam/waynfletestudies" rel="noopener" class="external-link" href="https://github.com/tahmidazam/waynfletestudies" target="_blank">repository</a>.</p></div><div><p>This project was presented on February 3rd, 2023, at Magdalen College School, Oxford at the <a data-tooltip-position="top" aria-label="https://www.mcsoxford.org/waynflete-studies-evening-2023/#:~:text=MCS%20welcomed%20attendees%20to%20our,certainly%20was%20something%20for%20everyone!" rel="noopener" class="external-link" href="https://www.mcsoxford.org/waynflete-studies-evening-2023/#:~:text=MCS%20welcomed%20attendees%20to%20our,certainly%20was%20something%20for%20everyone!" target="_blank">Waynflete Studies Evening</a>. Content from slides displayed in the presentation are included below in addition to project content.</p></div><div><p><em>This project is dedicated to my brother, Tahsin.</em></p></div><div><hr></div><div class="heading-wrapper"><h2 data-heading="Abstract" class="heading" id="Abstract"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>Abstract</h2><div class="heading-children"><div><p>Communication is a pillar of societal interaction for humans, pivotal from an early age. An incapacity to communicate is a universal obstruction to a child’s social and emotional development. This project aims to explore the impacts of speech, language, and communication needs, and how an augmentative and alternative communication system, specifically a brain-computer interface, can lessen them. Consumer electroencephalography systems for recording electrical activity in the brain are becoming more affordable. Additionally, the number of large datasets available is increasing, enabling neural networks to become a prospective option for motor imagery classification. Moreover, the high mobile computing performance and power efficiency of today’s devices can host these neural network classifiers. Both brain imaging and classification are integral to the workings of a brain-computer interface. The results of this project find that a unidirectional, long short-term memory, recurrent neural network can classify left- and right- hand imageries to an accuracy of 91.95%, demonstrating a high level of capability.</p></div></div></div><div class="heading-wrapper"><h2 data-heading="Introduction" class="heading" id="Introduction"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>Introduction</h2><div class="heading-children"><div><p>Speech, language, and communication needs (SLCN) describe an individual for whom spoken language is insufficient for supporting conversation and peer interaction <sup data-footnote-id="fnref-1-ca2cfb94fc5db613" class="footnote-ref" id="fnref-1-ca2cfb94fc5db613"><a href="#fn-1-ca2cfb94fc5db613" class="footnote-link" target="_self" rel="noopener">[1]</a></sup>. SLCN can arise from aphonia, verbal dyspraxia, learning disabilities like autism spectrum disorder, congenital conditions such as Down’s syndrome, and cancers or injury to the head or neck <sup data-footnote-id="fnref-2-ca2cfb94fc5db613" class="footnote-ref" id="fnref-2-ca2cfb94fc5db613"><a href="#fn-2-ca2cfb94fc5db613" class="footnote-link" target="_self" rel="noopener">[2]</a></sup>. A population study found that at school entry, 2 out of 30 children will experience SLCN severe enough to affect development and academic progress <sup data-footnote-id="fnref-3-ca2cfb94fc5db613" class="footnote-ref" id="fnref-3-ca2cfb94fc5db613"><a href="#fn-3-ca2cfb94fc5db613" class="footnote-link" target="_self" rel="noopener">[3]</a></sup>. In addition, more primary school children have SLCN than any other type of special educational need <sup data-footnote-id="fnref-4-ca2cfb94fc5db613" class="footnote-ref" id="fnref-4-ca2cfb94fc5db613"><a href="#fn-4-ca2cfb94fc5db613" class="footnote-link" target="_self" rel="noopener">[4]</a></sup>. The capacity for communication is the foundation for independence and participation in society <sup data-footnote-id="fnref-5-ca2cfb94fc5db613" class="footnote-ref" id="fnref-5-ca2cfb94fc5db613"><a href="#fn-5-ca2cfb94fc5db613" class="footnote-link" target="_self" rel="noopener">[5]</a></sup>; it is integral to social, emotional and educational development <sup data-footnote-id="fnref-6-ca2cfb94fc5db613" class="footnote-ref" id="fnref-6-ca2cfb94fc5db613"><a href="#fn-6-ca2cfb94fc5db613" class="footnote-link" target="_self" rel="noopener">[6]</a></sup> in addition to one’s sense of self and cultural identity <sup data-footnote-id="fnref-5-1-ca2cfb94fc5db613" class="footnote-ref" id="fnref-5-1-ca2cfb94fc5db613"><a href="#fn-5-ca2cfb94fc5db613" class="footnote-link" target="_self" rel="noopener">[5-1]</a></sup>. To mitigate SLCN, schools often employ differentiated curriculums and timetables that provide more focused teaching but reduce social involvement. Limiting the time an individual has with their peers can result in isolation, alienation, and inclusion issues in the classroom <sup data-footnote-id="fnref-1-1-ca2cfb94fc5db613" class="footnote-ref" id="fnref-1-1-ca2cfb94fc5db613"><a href="#fn-1-ca2cfb94fc5db613" class="footnote-link" target="_self" rel="noopener">[1-1]</a></sup>. These effects of SLCN carry into later life: studies demonstrate that SLCN is associated with poor literacy, mental health and employment outcomes <sup data-footnote-id="fnref-7-ca2cfb94fc5db613" class="footnote-ref" id="fnref-7-ca2cfb94fc5db613"><a href="#fn-7-ca2cfb94fc5db613" class="footnote-link" target="_self" rel="noopener">[7]</a></sup>. Figure 1 reveals the prevalence of SLCN throughout life and emphasises its dominance over early life.</p></div><div><p><span alt="Figure 1: The prevalence of SLCN [6]." src="Figure 1.jpg" class="internal-embed media-embed image-embed is-loaded" style="width: 300px; max-width: 100%;"><img alt="Figure 1: The prevalence of SLCN [6]." src="figures/figure-1.jpg" style="width: 300px; max-width: 100%;"></span></p></div><div><p>Augmentative and alternative communication (AAC) systems attempt to break down the barriers constructed by SLCN. AAC strategies are diverse, from the low-tech solutions of sign language and display boards to high-tech electronic systems relying on speech generation <sup data-footnote-id="fnref-8-ca2cfb94fc5db613" class="footnote-ref" id="fnref-8-ca2cfb94fc5db613"><a href="#fn-8-ca2cfb94fc5db613" class="footnote-link" target="_self" rel="noopener">[8]</a></sup>. There are a variety of these high-tech applications, from touchscreens and keyboards to those that use eye-tracking and breath activation <sup data-footnote-id="fnref-8-1-ca2cfb94fc5db613" class="footnote-ref" id="fnref-8-1-ca2cfb94fc5db613"><a href="#fn-8-ca2cfb94fc5db613" class="footnote-link" target="_self" rel="noopener">[8-1]</a></sup>, summarised in Slide 1.</p></div><div><p><span alt="Slide 1: 'Augmentative and alternative communication (AAC) systems  attempt to break down the barriers of SLCN'" src="Slide 1.jpeg" class="internal-embed media-embed image-embed is-loaded"><img alt="Slide 1: 'Augmentative and alternative communication (AAC) systems  attempt to break down the barriers of SLCN'" src="slides/slide-1.jpeg"></span></p></div><div><p>To gain an impression of the experience of current AAC systems, I conducted an interview for this study <sup data-footnote-id="fnref-9-ca2cfb94fc5db613" class="footnote-ref" id="fnref-9-ca2cfb94fc5db613"><a href="#fn-9-ca2cfb94fc5db613" class="footnote-link" target="_self" rel="noopener">[9]</a></sup> with the mother of Amos, a 9-year-old child diagnosed with Down’s Syndrome and developmental verbal dyspraxia (DVD). Down’s Syndrome is a genetic condition involving the trisomy of chromosome 21 <sup data-footnote-id="fnref-10-ca2cfb94fc5db613" class="footnote-ref" id="fnref-10-ca2cfb94fc5db613"><a href="#fn-10-ca2cfb94fc5db613" class="footnote-link" target="_self" rel="noopener">[10]</a></sup>. Verbal dyspraxia involves an impairment of speaking as a result of difficulties in muscle coordination <sup data-footnote-id="fnref-11-ca2cfb94fc5db613" class="footnote-ref" id="fnref-11-ca2cfb94fc5db613"><a href="#fn-11-ca2cfb94fc5db613" class="footnote-link" target="_self" rel="noopener">[11]</a></sup>. Amos communicates daily with Makaton sign language, which is a symbol and sign set <sup data-footnote-id="fnref-12-ca2cfb94fc5db613" class="footnote-ref" id="fnref-12-ca2cfb94fc5db613"><a href="#fn-12-ca2cfb94fc5db613" class="footnote-link" target="_self" rel="noopener">[12]</a></sup>; however, this method has become inadequate in recent years as his emotional and educational needs have grown complex. Sign language has been applicable for communication within Amos’ family but as interaction extends to his peers at his school, it becomes less effective as many are unfamiliar with sign language. To support his communication further, he utilises TouchChat, an AAC application for tablets, which uses a matrix of on-screen buttons and a speech generation function, enabling conversation with those around him. This method provides a great deal of convenience but also serves as a distraction due to the additional functions of the tablet. Despite the portable form factor, the tablet needs to be in-hand and carried everywhere. Without his AAC, Amos is little understood and often underestimated. The communication gap between him and his peers widens, ultimately leading to feelings of frustration.</p></div><div><p><span alt="Figure 2: Flowchart comparing the pathways of traditional and BCI AAC communication." src="Figure 2.png" class="internal-embed media-embed image-embed is-loaded"><img alt="Figure 2: Flowchart comparing the pathways of traditional and BCI AAC communication." src="figures/figure-2.png"></span><br>
In the context of AAC, the brain-computer interface (BCI) allows for control of a speech-generating device by modulating one’s brain signals <sup data-footnote-id="fnref-8-2-ca2cfb94fc5db613" class="footnote-ref" id="fnref-8-2-ca2cfb94fc5db613"><a href="#fn-8-ca2cfb94fc5db613" class="footnote-link" target="_self" rel="noopener">[8-2]</a></sup>, providing a universal and non-muscular channel for communication <sup data-footnote-id="fnref-13-ca2cfb94fc5db613" class="footnote-ref" id="fnref-13-ca2cfb94fc5db613"><a href="#fn-13-ca2cfb94fc5db613" class="footnote-link" target="_self" rel="noopener">[13]</a></sup>. As a result, they are undetectable in body language and require no peer training, which makes for a more seamless experience for children like Amos. The green channel in Figure 2 demonstrates how the neuromuscular coordination of vocal cords (the red channel) is bypassed by a BCI AAC system (the green channel). The process begins with the intent of the user, which triggers a cascade of complex processes involving the activation of many areas of the brain and the motor pathways of the nervous system <sup data-footnote-id="fnref-14-ca2cfb94fc5db613" class="footnote-ref" id="fnref-14-ca2cfb94fc5db613"><a href="#fn-14-ca2cfb94fc5db613" class="footnote-link" target="_self" rel="noopener">[14]</a></sup>. These activations, also called motor imagery, are observed via electroencephalography (EEG), a method of measuring the electrical activity in the brain <sup data-footnote-id="fnref-15-ca2cfb94fc5db613" class="footnote-ref" id="fnref-15-ca2cfb94fc5db613"><a href="#fn-15-ca2cfb94fc5db613" class="footnote-link" target="_self" rel="noopener">[15]</a></sup>. The BCI then picks out the relevant streams of intention amidst the sea of electrophysiological activity by pre-processing the signal, extracting features, and finally classifying them. Feature extraction splits the stream into salient snippets of signals. These snippets, or epochs, are mapped into outputs via classification, which finally control the speech-generating device <sup data-footnote-id="fnref-8-3-ca2cfb94fc5db613" class="footnote-ref" id="fnref-8-3-ca2cfb94fc5db613"><a href="#fn-8-ca2cfb94fc5db613" class="footnote-link" target="_self" rel="noopener">[8-3]</a></sup>. BCIs continuously read brain activity, and feature extraction, signal processing and classification all must occur in real-time. Currently, BCIs are used as a last resort for AAC systems <sup data-footnote-id="fnref-14-1-ca2cfb94fc5db613" class="footnote-ref" id="fnref-14-1-ca2cfb94fc5db613"><a href="#fn-14-ca2cfb94fc5db613" class="footnote-link" target="_self" rel="noopener">[14-1]</a></sup>. They are common in patients with locked-in syndrome (a state describing a person who is cognitively intact but paralysed, unable to perform voluntary movement <sup data-footnote-id="fnref-14-2-ca2cfb94fc5db613" class="footnote-ref" id="fnref-14-2-ca2cfb94fc5db613"><a href="#fn-14-ca2cfb94fc5db613" class="footnote-link" target="_self" rel="noopener">[14-2]</a></sup>). Amyotrophic lateral sclerosis is an example of a locked-in condition, where initial muscle atrophy develops to a complete loss of voluntary movement <sup data-footnote-id="fnref-8-4-ca2cfb94fc5db613" class="footnote-ref" id="fnref-8-4-ca2cfb94fc5db613"><a href="#fn-8-ca2cfb94fc5db613" class="footnote-link" target="_self" rel="noopener">[8-4]</a></sup>. Life support systems such as artificial respiration and artificial nutrition can prolong life expectancy, but the loss of motor pathways inhibits all forms of communication. The BCI becomes a locked-in human’s only form of interaction with the wider world. Today’s brain-computer interfaces are slow and unreliable <sup data-footnote-id="fnref-13-1-ca2cfb94fc5db613" class="footnote-ref" id="fnref-13-1-ca2cfb94fc5db613"><a href="#fn-13-ca2cfb94fc5db613" class="footnote-link" target="_self" rel="noopener">[13-1]</a></sup>, limiting their ability to support a wider range of needs. This project aims to challenge these limitations.</p></div><div><p>Classification is the crux of BCIs: it is the process by which brain activity is sorted into classes, for example left- or right-hand. Statistical classifiers, support vector machines, neural networks and nonlinear Bayesian classifiers <sup data-footnote-id="fnref-8-5-ca2cfb94fc5db613" class="footnote-ref" id="fnref-8-5-ca2cfb94fc5db613"><a href="#fn-8-ca2cfb94fc5db613" class="footnote-link" target="_self" rel="noopener">[8-5]</a></sup> are examples of algorithms that map brain activity to classes. Neural networks are excellent at pattern recognition; their method of learning from example is akin to the workings of the human brain. In BCIs, they are applicable as the defining patterns for different motor imageries are found deep in the rhythm of multiple electrodes and behind walls of signal noise and artefacts <sup data-footnote-id="fnref-13-2-ca2cfb94fc5db613" class="footnote-ref" id="fnref-13-2-ca2cfb94fc5db613"><a href="#fn-13-ca2cfb94fc5db613" class="footnote-link" target="_self" rel="noopener">[13-2]</a></sup><sup data-footnote-id="fnref-16-ca2cfb94fc5db613" class="footnote-ref" id="fnref-16-ca2cfb94fc5db613"><a href="#fn-16-ca2cfb94fc5db613" class="footnote-link" target="_self" rel="noopener">[16]</a></sup><sup data-footnote-id="fnref-17-ca2cfb94fc5db613" class="footnote-ref" id="fnref-17-ca2cfb94fc5db613"><a href="#fn-17-ca2cfb94fc5db613" class="footnote-link" target="_self" rel="noopener">[17]</a></sup>. Their potential is growing as large datasets are becoming available and mobile computing performance and power efficiency are improving <sup data-footnote-id="fnref-18-ca2cfb94fc5db613" class="footnote-ref" id="fnref-18-ca2cfb94fc5db613"><a href="#fn-18-ca2cfb94fc5db613" class="footnote-link" target="_self" rel="noopener">[18]</a></sup>. In this project, a neural network is trained from a dataset of EEG recordings, <sup data-footnote-id="fnref-19-ca2cfb94fc5db613" class="footnote-ref" id="fnref-19-ca2cfb94fc5db613"><a href="#fn-19-ca2cfb94fc5db613" class="footnote-link" target="_self" rel="noopener">[19]</a></sup>, to classify left- and right-hand imagery and then tested to assess its effectiveness. Slide 2 summarises the components of BCI-based communication.</p></div><div><p><span alt="Slide 1: 'Brain-computer interface offer a universal, non-muscular channel for communication'" src="Slide 2.jpeg" class="internal-embed media-embed image-embed is-loaded"><img alt="Slide 1: 'Brain-computer interface offer a universal, non-muscular channel for communication'" src="slides/slide-2.jpeg"></span></p></div></div></div><div class="heading-wrapper"><h2 data-heading="Electroencephalography" class="heading" id="Electroencephalography"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>Electroencephalography</h2><div class="heading-children"><div><p>Electroencephalography measures the electrical activity of the brain. When neurones transmit information between each other, current dipoles are formed by simultaneous postsynaptic potentials <sup data-footnote-id="fnref-20-ca2cfb94fc5db613" class="footnote-ref" id="fnref-20-ca2cfb94fc5db613"><a href="#fn-20-ca2cfb94fc5db613" class="footnote-link" target="_self" rel="noopener">[20]</a></sup>. These potentials are measured by electrodes placed on the scalp. Current dipoles are the configuration of intracellular current sources and extracellular current sinks that form following the release of neurotransmitters at the terminal boutons of axons at a synapse <sup data-footnote-id="fnref-15-1-ca2cfb94fc5db613" class="footnote-ref" id="fnref-15-1-ca2cfb94fc5db613"><a href="#fn-15-ca2cfb94fc5db613" class="footnote-link" target="_self" rel="noopener">[15-1]</a></sup>. The equipment used to carry out an EEG consists of electrically conductive electrodes, operational amplifiers, and an analogue-to-digital converter (ADC). Amplifiers increase the amplitude of the potentials measured by the electrodes so that the ADC can sample at suitably high precision. The output of an EEG system is a log of potential difference over time between active electrodes and the reference electrode.</p></div><div><p>EEG is chosen for BCIs as systems are relatively low-cost, portable and have a very high time resolution when compared to other brain imaging methods such as magnetoencephalography (MEG) and functional magnetic resonance imaging (fMRI) <sup data-footnote-id="fnref-21-ca2cfb94fc5db613" class="footnote-ref" id="fnref-21-ca2cfb94fc5db613"><a href="#fn-21-ca2cfb94fc5db613" class="footnote-link" target="_self" rel="noopener">[21]</a></sup>. High time resolution describes EEG’s high sample rate which allows measurements at millisecond time intervals. The non-invasive nature removes the risk of infection from percutaneous wires, in conjunction with issues of biocompatibility: tissue damage and reaction to the presence of electrodes inside the cranium can cause signal loss in the long-term <sup data-footnote-id="fnref-21-1-ca2cfb94fc5db613" class="footnote-ref" id="fnref-21-1-ca2cfb94fc5db613"><a href="#fn-21-ca2cfb94fc5db613" class="footnote-link" target="_self" rel="noopener">[21-1]</a></sup>. Portability results from active electrodes which have small on-scalp amplifiers that utilise wireless communication to transmit signals to a processing unit <sup data-footnote-id="fnref-21-2-ca2cfb94fc5db613" class="footnote-ref" id="fnref-21-2-ca2cfb94fc5db613"><a href="#fn-21-ca2cfb94fc5db613" class="footnote-link" target="_self" rel="noopener">[21-2]</a></sup>. Minimising the cables in the system prevents the issues of electrode dislocation and signal disruption from cable sway <sup data-footnote-id="fnref-21-3-ca2cfb94fc5db613" class="footnote-ref" id="fnref-21-3-ca2cfb94fc5db613"><a href="#fn-21-ca2cfb94fc5db613" class="footnote-link" target="_self" rel="noopener">[21-3]</a></sup>.</p></div><div><p>However, traditional EEG systems require uncomfortable electrolytic gels or salts to be applied, which provide a conductive path between the scalp and the electrode <sup data-footnote-id="fnref-14-3-ca2cfb94fc5db613" class="footnote-ref" id="fnref-14-3-ca2cfb94fc5db613"><a href="#fn-14-ca2cfb94fc5db613" class="footnote-link" target="_self" rel="noopener">[14-3]</a></sup>. Without gels, the inconsistency in resistance between the scalp and electrode introduces noise into the signal. As potentials must cross from brain tissue, the skull, and the scalp, the signal recorded at the electrode is prone to artefacts, which can originate from cardiac and ocular activity <sup data-footnote-id="fnref-15-2-ca2cfb94fc5db613" class="footnote-ref" id="fnref-15-2-ca2cfb94fc5db613"><a href="#fn-15-ca2cfb94fc5db613" class="footnote-link" target="_self" rel="noopener">[15-2]</a></sup>. The signal is also incredibly weak, with amplitudes in the microvolts <sup data-footnote-id="fnref-13-3-ca2cfb94fc5db613" class="footnote-ref" id="fnref-13-3-ca2cfb94fc5db613"><a href="#fn-13-ca2cfb94fc5db613" class="footnote-link" target="_self" rel="noopener">[13-3]</a></sup>, increasing its sensitivity to noise from external environmental disturbances and mains oscillations further <sup data-footnote-id="fnref-15-3-ca2cfb94fc5db613" class="footnote-ref" id="fnref-15-3-ca2cfb94fc5db613"><a href="#fn-15-ca2cfb94fc5db613" class="footnote-link" target="_self" rel="noopener">[15-3]</a></sup>. Extracranial EEG’s spatial resolution is poor compared to MEG, fMRI, and invasive methods <sup data-footnote-id="fnref-21-4-ca2cfb94fc5db613" class="footnote-ref" id="fnref-21-4-ca2cfb94fc5db613"><a href="#fn-21-ca2cfb94fc5db613" class="footnote-link" target="_self" rel="noopener">[21-4]</a></sup>. This can be attributed to the requirement of simultaneity of potentials for a sufficiently large amplitude able to travel to the scalp to be measured: each electrode’s signal is affected by roughly 100 million and 1 billion neurons <sup data-footnote-id="fnref-15-4-ca2cfb94fc5db613" class="footnote-ref" id="fnref-15-4-ca2cfb94fc5db613"><a href="#fn-15-ca2cfb94fc5db613" class="footnote-link" target="_self" rel="noopener">[15-4]</a></sup><sup data-footnote-id="fnref-22-ca2cfb94fc5db613" class="footnote-ref" id="fnref-22-ca2cfb94fc5db613"><a href="#fn-22-ca2cfb94fc5db613" class="footnote-link" target="_self" rel="noopener">[22]</a></sup>. This low spatial resolution makes differentiating between motor imageries that occur close together on the scalp impossible: a distinction between left and right foot imageries cannot be made with current EEG <sup data-footnote-id="fnref-14-4-ca2cfb94fc5db613" class="footnote-ref" id="fnref-14-4-ca2cfb94fc5db613"><a href="#fn-14-ca2cfb94fc5db613" class="footnote-link" target="_self" rel="noopener">[14-4]</a></sup>. Current BCI systems use four classes of movement: the hands, legs, feet, and tongue, and provide 2 bits of information per sample <sup data-footnote-id="fnref-13-4-ca2cfb94fc5db613" class="footnote-ref" id="fnref-13-4-ca2cfb94fc5db613"><a href="#fn-13-ca2cfb94fc5db613" class="footnote-link" target="_self" rel="noopener">[13-4]</a></sup>. As a result of this low bit depth, a menu system that hierarchically organises phrases into a navigable tree must be used, adding a great deal of complexity to a BCI-based AAC.</p></div></div></div><div class="heading-wrapper"><h2 data-heading="Neural networks" class="heading" id="Neural_networks"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>Neural networks</h2><div class="heading-children"><div><p>Neural networks are composed of neurones represented by nodes, and synapses represented by edges. Each node has a set of input nodes, <span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D463 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D457 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-math></mjx-container></span>, which are connected via edges, each with a weight, <span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D464 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D457 TEX-I"></mjx-c></mjx-mi><mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D457 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.289em;"><mjx-mi class="mjx-var" size="s"><mjx-c class="mjx-c2032"></mjx-c></mjx-mi></mjx-script></mjx-msup></mjx-texatom></mjx-script></mjx-msub></mjx-math></mjx-container></span>. Applying an activation function, <span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D459 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D457 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-math></mjx-container></span>, to a weighted sum of the input nodes results in the output of the node, <span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D463 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D457 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-math></mjx-container></span>. This manipulation is outlined in Slide 3.</p></div><div><p><span class="math math-block is-loaded"><mjx-container class="MathJax" jax="CHTML" display="true"><mjx-math display="true" class="MJX-TEX" style="margin-left: 0px; margin-right: 0px;"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D463 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c2212"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="3"><mjx-c class="mjx-c1D457 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-msub space="4"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D459 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D457 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-s4"><mjx-c class="mjx-c5B TEX-S4"></mjx-c></mjx-mo><mjx-munder><mjx-row><mjx-base><mjx-mo class="mjx-lop"><mjx-c class="mjx-c2211 TEX-S2"></mjx-c></mjx-mo></mjx-base></mjx-row><mjx-row><mjx-under style="padding-top: 0.167em; padding-left: 0.478em;"><mjx-texatom size="s" texclass="ORD"><mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D457 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.289em;"><mjx-mi class="mjx-var" size="s"><mjx-c class="mjx-c2032"></mjx-c></mjx-mi></mjx-script></mjx-msup></mjx-texatom></mjx-under></mjx-row></mjx-munder><mjx-texatom space="2" texclass="ORD"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D464 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D457 TEX-I"></mjx-c></mjx-mi><mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D457 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.289em;"><mjx-mi class="mjx-var" size="s"><mjx-c class="mjx-c2032"></mjx-c></mjx-mi></mjx-script></mjx-msup></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo><mjx-msub space="3"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D463 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D457 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.289em;"><mjx-mi class="mjx-var" size="s"><mjx-c class="mjx-c2032"></mjx-c></mjx-mi></mjx-script></mjx-msup></mjx-texatom></mjx-script></mjx-msub></mjx-texatom><mjx-mo class="mjx-s4"><mjx-c class="mjx-c5D TEX-S4"></mjx-c></mjx-mo></mjx-mrow></mjx-math></mjx-container></span></p></div><div><p><span alt="Slide 2: 'Neural network structure mimics interconnected web of neurones in our brains'" src="Slide 3.jpeg" class="internal-embed media-embed image-embed is-loaded"><img alt="Slide 2: 'Neural network structure mimics interconnected web of neurones in our brains'" src="slides/slide-3.jpeg"></span></p></div><div><p>Examples of activation functions include the sigmoid, tanh, softmax and the rectified linear unit (ReLU). This flow of data and sequence of functions are outlined in Figure 3. </p></div><div><p><span alt="Figure 3: Data flow through a node of a neural network [23]." src="Figure 3.png" class="internal-embed media-embed image-embed is-loaded" style="width: 400px; max-width: 100%;"><img alt="Figure 3: Data flow through a node of a neural network [23]." src="figures/figure-3.png" style="width: 400px; max-width: 100%;"></span></p></div><div><p>Nodes form layers, which form an overall network. They can be described as feed-forward, where the outputs of one layer feed into the inputs of the next layer. This network of nodes parallels the interconnected web of neurones found in our brains <sup data-footnote-id="fnref-18-1-ca2cfb94fc5db613" class="footnote-ref" id="fnref-18-1-ca2cfb94fc5db613"><a href="#fn-18-ca2cfb94fc5db613" class="footnote-link" target="_self" rel="noopener">[18-1]</a></sup>. This structure can be seen in Figure 4, Slide 4, and Slide 5.</p></div><div><p><span alt="Figure 4: Structure of a feed-forward network of layers [23]." src="Figure 4.png" class="internal-embed media-embed image-embed is-loaded" style="width: 250px; max-width: 100%;"><img alt="Figure 4: Structure of a feed-forward network of layers [23]." src="figures/figure-4.png" style="width: 250px; max-width: 100%;"></span></p></div><div><p><span alt="Slide 4: 'Data is passed forward through layers of nodes'" src="Slide 4.jpeg" class="internal-embed media-embed image-embed is-loaded"><img alt="Slide 4: 'Data is passed forward through layers of nodes'" src="slides/slide-4.jpeg"></span></p></div><div><p><span alt="Slide 5: 'The neural network takes electrodes as input, hidden layers process their data and output a confidence for each class'" src="Slide 5.jpeg" class="internal-embed media-embed image-embed is-loaded"><img alt="Slide 5: 'The neural network takes electrodes as input, hidden layers process their data and output a confidence for each class'" src="slides/slide-5.jpeg"></span></p></div><div><p>Neural networks have an inherent advantage over programmer-controlled threshold-based classifiers because they can learn. They achieve this by changing the weights of the edges in the networks until the network classifies data correctly in a process called training. A loss function punishes the ‘distance’ between a network’s output and the correct classification output. It is minimised by gradient descent, where weights are altered by backpropagation, a process that involves the derivative of the loss function with respect to the weights of the edges in the network being used to find a solution of higher accuracy of the network <sup data-footnote-id="fnref-23-ca2cfb94fc5db613" class="footnote-ref" id="fnref-23-ca2cfb94fc5db613"><a href="#fn-23-ca2cfb94fc5db613" class="footnote-link" target="_self" rel="noopener">[23]</a></sup>.</p></div><div><p>A flaw of conventional neural networks in the context of EEG interpretation is their assumption of independence for each data point <sup data-footnote-id="fnref-24-ca2cfb94fc5db613" class="footnote-ref" id="fnref-24-ca2cfb94fc5db613"><a href="#fn-24-ca2cfb94fc5db613" class="footnote-link" target="_self" rel="noopener">[24]</a></sup>. It would be difficult to determine how much the rhythms of motor activity related oscillations are suppressed or enhanced with only an instantaneous point in the dataset and no history of its past or future points. In conventional neural networks, the state of the network is lost every cycle, making connections between data points differing in time impossible. It would be much easier to distinguish rhythm if a range of data points were compared, or if the network looked at changes over time, to form an impression of longer-term patterns.</p></div><div><p>Recurrent neural networks are a type of neural network that feature edges spanning adjacent steps, or over cycles of the network, which allow information to be passed over time. These edges are shown in Figure 5 as the edges connected nodes across time steps together. </p></div><div><p><span alt="Figure 5: Data flow through a recurrent neural network [23]." src="Figure 5.png" class="internal-embed media-embed image-embed is-loaded" style="width: 250px; max-width: 100%;"><img alt="Figure 5: Data flow through a recurrent neural network [23]." src="figures/figure-5.png" style="width: 250px; max-width: 100%;"></span></p></div><div><p><span alt="Slide 6: 'Recurrent neural networks connect the network across time steps'" src="Slide 6.jpeg" class="internal-embed media-embed image-embed is-loaded"><img alt="Slide 6: 'Recurrent neural networks connect the network across time steps'" src="slides/slide-6.jpeg"></span></p></div><div><p>However, long term dependencies are difficult to learn because gradients vanish and explode when backpropagating through many steps in time. Long short-term memory (LSTM) networks are a type of recurrent neural network that features the memory cell that replaces every node <sup data-footnote-id="fnref-25-ca2cfb94fc5db613" class="footnote-ref" id="fnref-25-ca2cfb94fc5db613"><a href="#fn-25-ca2cfb94fc5db613" class="footnote-link" target="_self" rel="noopener">[25]</a></sup>. </p></div><div><p>The memory cell functions to overcome vanishing and exploding gradients by including a self- connected recurrent edge of fixed weight one shown by the blue arrow in Figure 6, suitably terminating runaway derivative calculations <sup data-footnote-id="fnref-23-1-ca2cfb94fc5db613" class="footnote-ref" id="fnref-23-1-ca2cfb94fc5db613"><a href="#fn-23-ca2cfb94fc5db613" class="footnote-link" target="_self" rel="noopener">[23-1]</a></sup>. </p></div><div><p><span alt="Figure 6: The LSTM memory cell [24]." src="Figure 6.png" class="internal-embed media-embed image-embed is-loaded" style="width: 250px; max-width: 100%;"><img alt="Figure 6: The LSTM memory cell [24]." src="figures/figure-6.png" style="width: 250px; max-width: 100%;"></span></p></div><div><p>Long short-term memory networks can also be bidirectional, featuring recurrent edges both forwards in time and backwards in time <sup data-footnote-id="fnref-23-2-ca2cfb94fc5db613" class="footnote-ref" id="fnref-23-2-ca2cfb94fc5db613"><a href="#fn-23-ca2cfb94fc5db613" class="footnote-link" target="_self" rel="noopener">[23-2]</a></sup>.</p></div></div></div><div class="heading-wrapper"><h2 data-heading="Experimental method" class="heading" id="Experimental_method"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>Experimental method</h2><div class="heading-children"><div><p>To demonstrate the capability of neural networks in accurately classifying motor imagery for applications in a BCI AAC, a long short-term memory, recurrent neural network was trained, optimised, and tested.</p></div><div class="heading-wrapper"><h3 data-heading="Compiling a dataset" class="heading" id="Compiling_a_dataset"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>Compiling a dataset</h3><div class="heading-children"><div><p>To form a dataset for network training, <sup data-footnote-id="fnref-19-1-ca2cfb94fc5db613" class="footnote-ref" id="fnref-19-1-ca2cfb94fc5db613"><a href="#fn-19-ca2cfb94fc5db613" class="footnote-link" target="_self" rel="noopener">[19-1]</a></sup>, a motor imagery dataset was used. In this study 13 participants each sat facing a screen for approximately 1 hour with an EEG system recording their brain’s electrical activity. The screen hosted a graphical user interface (GUI) which provided the participant with commands to perform periodically. The state of the GUI was stored alongside each sample taken by the EEG to a marker file, forming a historical reference of all commands displayed to the participant. There were several different paradigms each with a different experimental design. For example, one involved displaying prompts to move different fingers on each hand, another involved the hands, legs, feet, and tongue. However, for this project, the left- and right-hand (L/R) paradigm was chosen as the cortical regions related to L/R motor imagery were the furthest apart on the scalp <sup data-footnote-id="fnref-14-5-ca2cfb94fc5db613" class="footnote-ref" id="fnref-14-5-ca2cfb94fc5db613"><a href="#fn-14-ca2cfb94fc5db613" class="footnote-link" target="_self" rel="noopener">[14-5]</a></sup>. This choice aims to maximise the distinguishing features that the neural network can exploit in the hopes of increasing classification performance. A BCI should classify imagined movement, so utilising recordings of executed movement may seem unproductive. However, <sup data-footnote-id="fnref-26-ca2cfb94fc5db613" class="footnote-ref" id="fnref-26-ca2cfb94fc5db613"><a href="#fn-26-ca2cfb94fc5db613" class="footnote-link" target="_self" rel="noopener">[26]</a></sup>, a study in which imagined touch sensations were found to yield real tactile feedback in the brain, demonstrates that executed movements have similar motor imagery to imagined movements. As a result, it is realistic to assume a neural network capable of classifying executed motor imagery can classify imagined motor imagery.</p></div><div><p>The processing of the EEG recordings was done in the MATLAB integrated development environment <sup data-footnote-id="fnref-27-ca2cfb94fc5db613" class="footnote-ref" id="fnref-27-ca2cfb94fc5db613"><a href="#fn-27-ca2cfb94fc5db613" class="footnote-link" target="_self" rel="noopener">[27]</a></sup> with a third-party toolbox called EEGLAB <sup data-footnote-id="fnref-28-ca2cfb94fc5db613" class="footnote-ref" id="fnref-28-ca2cfb94fc5db613"><a href="#fn-28-ca2cfb94fc5db613" class="footnote-link" target="_self" rel="noopener">[28]</a></sup> utilised for handling EEG data. The dataset compilation pipeline involves electrode indexing, filtering, artefact correction, independent component analysis and epoching. It is carried out by the code attached in Figure 20 and has parameters that control the pre-processing steps outlined below.</p></div></div></div><div class="heading-wrapper"><h3 data-heading="Electrode indexing" class="heading" id="Electrode_indexing"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>Electrode indexing</h3><div class="heading-children"><div><p>EEG research has a standard system for electrode placement to ensure consistency called the International 10-20 System <sup data-footnote-id="fnref-14-6-ca2cfb94fc5db613" class="footnote-ref" id="fnref-14-6-ca2cfb94fc5db613"><a href="#fn-14-ca2cfb94fc5db613" class="footnote-link" target="_self" rel="noopener">[14-6]</a></sup>, where electrode labels correspond to specific locations on the scalp. The dataset uses an EEG system with 21 electrodes (Fp1, Fp2, F3, F4, C3, C4, P3, P4, O1, O2, A1, A2, F7, F8, T3, T4, T5, T6, Fz, Cz, Pz). The locations of these electrodes on the scalp are displayed in Figure 7, and are sourced from an MNI coordinate file packaged with EEGLAB <sup data-footnote-id="fnref-29-ca2cfb94fc5db613" class="footnote-ref" id="fnref-29-ca2cfb94fc5db613"><a href="#fn-29-ca2cfb94fc5db613" class="footnote-link" target="_self" rel="noopener">[29]</a></sup>.</p></div><div><p><span alt="Figure 7: Locations of electrodes in [18], according to the 10-20 International System [13]." src="Figure 7.png" class="internal-embed media-embed image-embed is-loaded" style="width: 400px; max-width: 100%;"><img alt="Figure 7: Locations of electrodes in [18], according to the 10-20 International System [13]." src="figures/figure-7.png" style="width: 400px; max-width: 100%;"></span></p></div><div><p>Electrodes outside the bounds of the scalp pictured in Figure 7 are located on the sides of the head. These locations aid for pre- processing steps that utilise spatial relationships between signals, for example in independent component analysis and artefact correction. Note that electrode indexing occurs after ICA and artefact correction for this reason. Electrode indexing refers to which electrode signals contribute to the final dataset. Removing certain electrodes from the samples in the dataset functions to remove redundant data. Redundant data, for example, electrodes that do not display the relevant motor imagery, adds unnecessary complexity in the sample for the neural network to parse through, hindering classification accuracy.</p></div><div><p>Research has shown that discriminant motor imagery for L/R hand movements occurs in electrodes C3 and C4 <sup data-footnote-id="fnref-19-2-ca2cfb94fc5db613" class="footnote-ref" id="fnref-19-2-ca2cfb94fc5db613"><a href="#fn-19-ca2cfb94fc5db613" class="footnote-link" target="_self" rel="noopener">[19-2]</a></sup>. Specifically, the hands trigger imageries in the opposite lobe: the right hand in C3 and the left in C4 <sup data-footnote-id="fnref-14-7-ca2cfb94fc5db613" class="footnote-ref" id="fnref-14-7-ca2cfb94fc5db613"><a href="#fn-14-ca2cfb94fc5db613" class="footnote-link" target="_self" rel="noopener">[14-7]</a></sup>, visualised in Slide 7.</p></div><div><p><span alt="Slide 7: 'Color-based visualisation helps us recognise patterns in EEG recordings'" src="Slide 7.gif" class="internal-embed media-embed image-embed is-loaded"><img alt="Slide 7: 'Color-based visualisation helps us recognise patterns in EEG recordings'" src="slides/slide-7.gif"></span></p></div><div><p>Event-related potentials (ERPs) are plots of the average potential over time for L/R samples. To investigate which electrodes showed discriminant activity, ERPs were plotted from 0 to 0.5 seconds, with 0 seconds referring to the instant the GUI displayed a prompt. In line with research presented in <sup data-footnote-id="fnref-14-8-ca2cfb94fc5db613" class="footnote-ref" id="fnref-14-8-ca2cfb94fc5db613"><a href="#fn-14-ca2cfb94fc5db613" class="footnote-link" target="_self" rel="noopener">[14-8]</a></sup><sup data-footnote-id="fnref-19-3-ca2cfb94fc5db613" class="footnote-ref" id="fnref-19-3-ca2cfb94fc5db613"><a href="#fn-19-ca2cfb94fc5db613" class="footnote-link" target="_self" rel="noopener">[19-3]</a></sup>, electrodes C3 and C4 were the key differentiators between L/R imageries. This can be found in Figure 8 and 9 where electrode C4 features a negative peak in the left- hand imagery, and C3 features a positive peak, and vice versa for the right hand.</p></div><div><p><span alt="Figure 8: ERP of each electrode for L/R imagery" src="Figure 8.png" class="internal-embed media-embed image-embed is-loaded"><img alt="Figure 8: ERP of each electrode for L/R imagery" src="figures/figure-8.png"></span></p></div><div><p><span alt="Figure 9: ERP for L/R imagery in 3-dimensions" src="Figure 9.png" class="internal-embed media-embed image-embed is-loaded"><img alt="Figure 9: ERP for L/R imagery in 3-dimensions" src="figures/figure-9.png"></span></p></div><div><p>Other electrodes could be used to differentiate between L/R imageries: both F8 and Fp2 exhibit a decrease in amplitude from the left to right hand. However, a flip in polarity offers more utility and resilience to variations in peak amplitude in the epoch in the context of classification.</p></div></div></div><div class="heading-wrapper"><h3 data-heading="Filtering" class="heading" id="Filtering"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>Filtering</h3><div class="heading-children"><div><p>A bandpass filter of 0.5-90 Hz is applied in to remove direct current shifts, in conjunction with a notch filter at 50 Hz to remove mains line noise <sup data-footnote-id="fnref-17-1-ca2cfb94fc5db613" class="footnote-ref" id="fnref-17-1-ca2cfb94fc5db613"><a href="#fn-17-ca2cfb94fc5db613" class="footnote-link" target="_self" rel="noopener">[17-1]</a></sup>. Filtering refers to the attenuation of certain frequencies by decreasing their amplitudes of oscillation. The bandpass aspect means that the frequencies between 0.5 and 90 Hz (i.e., the band) are unaffected and allowed to ‘pass’, but other frequencies are diminished. The notch filter removes oscillations in the ‘notch’ of 50Hz, common in the mains power supply.</p></div></div></div><div class="heading-wrapper"><h3 data-heading="$\mu$ and $\beta$ frequency isolation" class="heading" id="$\mu$_and_$\beta$_frequency_isolation"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D707 TEX-I"></mjx-c></mjx-mi></mjx-math></mjx-container></span> and <span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D6FD TEX-I"></mjx-c></mjx-mi></mjx-math></mjx-container></span> frequency isolation</h3><div class="heading-children"><div><p>Sensorimotor rhythms of the brain relate to the preparation, control (and execution) of voluntary motion <sup data-footnote-id="fnref-16-1-ca2cfb94fc5db613" class="footnote-ref" id="fnref-16-1-ca2cfb94fc5db613"><a href="#fn-16-ca2cfb94fc5db613" class="footnote-link" target="_self" rel="noopener">[16-1]</a></sup>. The potentials of the brain are classified by their frequency and are split into five bands: delta, theta, mu, beta, and gamma activity. Sensorimotor rhythms (SMR) fall predominantly under alpha and beta activity <sup data-footnote-id="fnref-13-5-ca2cfb94fc5db613" class="footnote-ref" id="fnref-13-5-ca2cfb94fc5db613"><a href="#fn-13-ca2cfb94fc5db613" class="footnote-link" target="_self" rel="noopener">[13-5]</a></sup>. During the brain’s preparation for movement, a distinctive desynchronisation between the mu and beta rhythms takes place, called event-related desynchronisation (ERD). After movement is executed, these rhythms synchronise again, through event-related synchronisation (ERS) <sup data-footnote-id="fnref-17-2-ca2cfb94fc5db613" class="footnote-ref" id="fnref-17-2-ca2cfb94fc5db613"><a href="#fn-17-ca2cfb94fc5db613" class="footnote-link" target="_self" rel="noopener">[17-2]</a></sup>. These processes can be thought of as suppression and enhancement of rhythm <sup data-footnote-id="fnref-13-6-ca2cfb94fc5db613" class="footnote-ref" id="fnref-13-6-ca2cfb94fc5db613"><a href="#fn-13-ca2cfb94fc5db613" class="footnote-link" target="_self" rel="noopener">[13-6]</a></sup>. A second round of filtering, a bandpass of 8 to 30 Hz, functions to isolate the mu and beta frequencies in which these rhythms modulate.</p></div><div><p>The ERP plot function in EEGLAB also determines the peak potential for each imagery, considering all the electrodes in the sample. The peak potential is shown in the plots of Figure 10 as the blue x-intercept.</p></div><div><p><span alt="Figure 10: ERPs of L/R imageries with and without mu and beta frequency isolation." src="Figure 10.png" class="internal-embed media-embed image-embed is-loaded"><img alt="Figure 10: ERPs of L/R imageries with and without mu and beta frequency isolation." src="figures/figure-10.png"></span></p></div><div><p>At this peak potential, a coloured scalp map is also be rendered, based on the amplitudes of the electrodes and their locations on the scalp. They are good estimates of the timestamp at which the imagery relating to a movement takes place. On inspection of ERPs of L/R imageries with and without mu and beta frequency isolation, the pattern of a distinct negative peak in the opposite lobe to the hand became less obvious. The delta between the calculated peak potential in Figure 10 turns out to be 20 times larger when mu and beta frequencies were isolated (<span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-mi class="mjx-n"><mjx-c class="mjx-c394"></mjx-c></mjx-mi><mjx-texatom texclass="ORD"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mover><mjx-over style="padding-bottom: 0.105em; padding-left: 0.486em; margin-bottom: -0.544em;"><mjx-mo class="mjx-n" style="width: 0px; margin-left: -0.25em;"><mjx-c class="mjx-cAF"></mjx-c></mjx-mo></mjx-over><mjx-base><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D443 TEX-I"></mjx-c></mjx-mi></mjx-base></mjx-mover></mjx-texatom><mjx-script style="vertical-align: -0.15em; margin-left: -0.109em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-texatom><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3A"></mjx-c></mjx-mo><mjx-mi class="mjx-n" space="4"><mjx-c class="mjx-c394"></mjx-c></mjx-mi><mjx-texatom texclass="ORD"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mover><mjx-over style="padding-bottom: 0.105em; padding-left: 0.486em; margin-bottom: -0.544em;"><mjx-mo class="mjx-n" style="width: 0px; margin-left: -0.25em;"><mjx-c class="mjx-cAF"></mjx-c></mjx-mo></mjx-over><mjx-base><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D443 TEX-I"></mjx-c></mjx-mi></mjx-base></mjx-mover></mjx-texatom><mjx-script style="vertical-align: -0.15em; margin-left: -0.109em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-texatom><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c2248"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="4"><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c30"></mjx-c></mjx-mn><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3A"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="4"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-math></mjx-container></span>). Furthermore, on initial training of the neural network with mu and beta frequency isolation applied to all the samples, the training accuracy stayed constant at 50%. This suggests that the isolation removed the differentiating features of L/R imageries and as a result, isolation was not utilised for future training.</p></div></div></div><div class="heading-wrapper"><h3 data-heading="Artefact correction" class="heading" id="Artefact_correction"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>Artefact correction</h3><div class="heading-children"><div><p>Artefact correction is a method by which bad portions of data are transformed into more sensible representations of the signal. EEGLAB’s artefact subspace reconstruction (ASR) algorithm can remove or correct these bad portions. It achieves this by first locating clean portions of the data and their standard deviations, 𝜎. It then finds portions of data that exceed 𝜎 by a certain factor (in this case 20 times) and finally transforms the signal into a better statistical fit, using 𝜎 as context <sup data-footnote-id="fnref-30-ca2cfb94fc5db613" class="footnote-ref" id="fnref-30-ca2cfb94fc5db613"><a href="#fn-30-ca2cfb94fc5db613" class="footnote-link" target="_self" rel="noopener">[30]</a></sup>. In this project, ASR’s artefact removal function was not used, the samples were instead corrected to maximise training data available to the network.</p></div></div></div><div class="heading-wrapper"><h3 data-heading="Independent component analysis" class="heading" id="Independent_component_analysis"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>Independent component analysis</h3><div class="heading-children"><div><p>Independent component analysis (ICA) separates a singular distinctive source mixed into multiple electrodes to further remove artefacts <sup data-footnote-id="fnref-31-ca2cfb94fc5db613" class="footnote-ref" id="fnref-31-ca2cfb94fc5db613"><a href="#fn-31-ca2cfb94fc5db613" class="footnote-link" target="_self" rel="noopener">[31]</a></sup><sup data-footnote-id="fnref-32-ca2cfb94fc5db613" class="footnote-ref" id="fnref-32-ca2cfb94fc5db613"><a href="#fn-32-ca2cfb94fc5db613" class="footnote-link" target="_self" rel="noopener">[32]</a></sup>. It involves running a decomposition process on the data and then removing any unwanted components once complete <sup data-footnote-id="fnref-17-3-ca2cfb94fc5db613" class="footnote-ref" id="fnref-17-3-ca2cfb94fc5db613"><a href="#fn-17-ca2cfb94fc5db613" class="footnote-link" target="_self" rel="noopener">[17-3]</a></sup>. The function of ICA is to ensure each electrode is an accurate representation of brain potentials of its area of scalp only and not a combination of multiple neighbouring areas. However, the decomposition process was time-intensive, taking minutes for each hour-long EEG recording, and running ICA per sample did not provide enough data for effective decomposition. For this reason, ICA was not used in the dataset formation pipeline.</p></div></div></div><div class="heading-wrapper"><h3 data-heading="Epoching and time-locking" class="heading" id="Epoching_and_time-locking"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>Epoching and time-locking</h3><div class="heading-children"><div><p>The hour-long recordings were converted into epochs or samples by splicing the recording and time- locking the splices to the first frame that a command was displayed. The codes for GUI state in the marker file are either 1, 2 or 3. These codes refer to left-hand, right-hand and a passive/neutral state <sup data-footnote-id="fnref-19-4-ca2cfb94fc5db613" class="footnote-ref" id="fnref-19-4-ca2cfb94fc5db613"><a href="#fn-19-ca2cfb94fc5db613" class="footnote-link" target="_self" rel="noopener">[19-4]</a></sup>. Since we plan to demonstrate the classification performance of left- and right-hand imagery, the passive/neutral state can be discarded. The start and end of each sample relative to the timestamp of the first frame the GUI changed (i.e., the timestamp at which the participant was shown a prompt) can be optimised via inspecting ERPs to minimise redundant data. The peak potential of the ERPs of L/R imagery without mu and beta isolation in Figure 10 occurs 100 ms after the average human reaction time of 267 ms <sup data-footnote-id="fnref-33-ca2cfb94fc5db613" class="footnote-ref" id="fnref-33-ca2cfb94fc5db613"><a href="#fn-33-ca2cfb94fc5db613" class="footnote-link" target="_self" rel="noopener">[33]</a></sup> and begins to fluctuate from the amplitude floor within 10 ms. The start and end timestamps for the epochs were chosen to be at 200 and 600 ms as a result. There are two peaks in the ERP for the L/R imageries: one at around 300 ms and another around 1300 to 1500 ms. The first peak’s higher amplitude, like electrode selection, provides a greater deal of resilience to variation across samples and was chosen for time-locking as a result.</p></div></div></div><div class="heading-wrapper"><h3 data-heading="Network setup and optimisation" class="heading" id="Network_setup_and_optimisation"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>Network setup and optimisation</h3><div class="heading-children"><div><p>As EEG data, the input into the neural network is a sequence of data points, a long short-term memory, recurrent neural network was chosen for the demonstration. Network performance can be quantified by a validation set. The validation set is a portion of the whole dataset that is hidden from the network, or, in other words, a portion of the data that is unseen during training. The validation set helps to highlight how the classification of unfamiliar samples fares. Periodically, the network classifies the validation set to obtain a validation accuracy, which is plotted alongside training accuracy. The closeness between these two accuracies is a good determiner of how well the network can generalise the patterns in samples. The layers in the network are as follows: a sequence input layer, a long short-term memory layer, a dropout layer, a fully connected layer, a softmax layer, and a classification layer. The data flow through these layers is outlined in Figure 11, and the function handling the training and testing is attached in Figure 21.</p></div><div><p><span alt="Figure 11: Neural network structure flowchart." src="Figure 11.png" class="internal-embed media-embed image-embed is-loaded" style="width: 250px; max-width: 100%;"><img alt="Figure 11: Neural network structure flowchart." src="figures/figure-11.png" style="width: 250px; max-width: 100%;"></span></p></div></div></div><div class="heading-wrapper"><h3 data-heading="The dropout layer" class="heading" id="The_dropout_layer"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>The dropout layer</h3><div class="heading-children"><div><p>Training the network without the dropout layer yielded accuracies of 72.22% for a single participant dataset and 67.82% for a 13-participant dataset. The training progress in Figure 12 highlights how the validation and training accuracies diverge, suggesting that the network is overfitting the training data.</p></div><div><p><span alt="Figure 12: Network training progress without a dropout layer, with a 13-participant dataset." src="Figure 12.png" class="internal-embed media-embed image-embed is-loaded"><img alt="Figure 12: Network training progress without a dropout layer, with a 13-participant dataset." src="figures/figure-12.png"></span></p></div><div><p>Overfitting is a lack of generalisation in the network, where the network almost memorises the training dataset samples rather than making use of wider patterns. This memorisation obstructs the validation classification accuracy from following the gains in training accuracy. The dropout layer can help minimise overfitting by probabilistically turning nodes to zero to limit the effect of a singular sample’s pass through the network. To find the optimal probability for the dropout layer, the parameter was iterated, tabulated in Figure 19, and graphed in Figure 13. The results reveal that the optimal dropout layer probability is 0.38. This value was calculated from the maxima of the quadratic trendline. At low probabilities, the network overfits; at high ones the network struggles to learn completely as all nodes are set to zero.</p></div><div><p><span alt="Figure 13: Graph of average network accuracy against dropout layer probability." src="Figure 13.png" class="internal-embed media-embed image-embed is-loaded"><img alt="Figure 13: Graph of average network accuracy against dropout layer probability." src="figures/figure-13.png"></span></p></div></div></div><div class="heading-wrapper"><h3 data-heading="The long short-term memory layer" class="heading" id="The_long_short-term_memory_layer"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>The long short-term memory layer</h3><div class="heading-children"><div><p>The long short-term memory layer has a parameter for the number of hidden units that governs the number of time steps the memory cell looks back on before feeding data forward. They can also be bidirectional, where the memory cell looks forward on future data as well. Iteration of this parameter and testing of the network was carried out to find the optimal number of hidden units, tabulated in Figure 18, and graphed in Figure 14.</p></div><div><p><span alt="Figure 14: Graph of average network accuracy against the number of hidden units." src="Figure 14.png" class="internal-embed media-embed image-embed is-loaded"><img alt="Figure 14: Graph of average network accuracy against the number of hidden units." src="figures/figure-14.png"></span></p></div><div><p>The closeness of the two trendlines demonstrates that the directionality of the LSTM layer had little effect on network performance. Unidirectional LSTM has the added benefit of being less computationally intensive and was chosen for future training as a result. At a low number of hidden units, the network accuracy is very low, close to random chance. At this low number of hidden units, the LSTM is not taking historical data points into account, and the network behaves like a conventional one, with no memory cells. This consolidates the idea that the LSTM layer is integral to classification. As the number of hidden units increases, the average network accuracy also increases. However, as the number of hidden units increases, classification becomes more time-intensive due to extra computation, so a balance of performance and speed of 75 hidden units was chosen for future training.</p></div></div></div><div class="heading-wrapper"><h3 data-heading="The softmax layer" class="heading" id="The_softmax_layer"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>The softmax layer</h3><div class="heading-children"><div><p>The softmax layer applies the softmax activation function, <span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D446 TEX-I"></mjx-c></mjx-mi></mjx-math></mjx-container></span>, to the output from the fully connected layer, <span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D457 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-math></mjx-container></span>. </p></div><div><p><span class="math math-block is-loaded"><mjx-container class="MathJax" jax="CHTML" display="true"><mjx-math display="true" class="MJX-TEX" style="margin-left: 0px; margin-right: 0px;"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D446 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D457 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mfrac space="4"><mjx-frac type="d"><mjx-num><mjx-nstrut type="d"></mjx-nstrut><mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.363em;"><mjx-texatom size="s" texclass="ORD"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D457 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-texatom></mjx-script></mjx-msup></mjx-num><mjx-dbox><mjx-dtable><mjx-line type="d"></mjx-line><mjx-row><mjx-den><mjx-dstrut type="d"></mjx-dstrut><mjx-mrow><mjx-mo class="mjx-sop"><mjx-c class="mjx-c2211 TEX-S1"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.319em;"><mjx-texatom size="s" texclass="ORD"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D457 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-texatom></mjx-script></mjx-msup></mjx-texatom></mjx-mrow></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac></mjx-math></mjx-container></span></p></div><div><p>The softmax function involves the application of the exponential function to each incoming input from the fully connected layer. Then normalisation is achieved by dividing by the sum of the exponentials of the input layers <sup data-footnote-id="fnref-34-ca2cfb94fc5db613" class="footnote-ref" id="fnref-34-ca2cfb94fc5db613"><a href="#fn-34-ca2cfb94fc5db613" class="footnote-link" target="_self" rel="noopener">[34]</a></sup>. The output of the classification layer is either a code 1 or a 2, which refer to the GUI states of a left-hand command or a right-hand command.</p></div></div></div><div class="heading-wrapper"><h3 data-heading="Early stopping" class="heading" id="Early_stopping"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>Early stopping</h3><div class="heading-children"><div><p>To reduce overfitting, early stopping was applied alongside the use of a dropout layer. It functions by halting training if the validation and training progress diverge. If the validation loss repeatedly does not decrease for a certain number of iterations of the training set (the validation patience), then the network training is stopped.</p></div></div></div></div></div><div class="heading-wrapper"><h2 data-heading="Results" class="heading" id="Results"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>Results</h2><div class="heading-children"><div><p>The unidirectional, long short-term memory, recurrent neural network post-optimisation yielded an accuracy of 77.01% in Figure 15 on a 13-participant dataset and 91.95% in Figure 16 on a single participant dataset.</p></div><div><p><span alt="Figure 15: Network training progress post-optimisation with a 13-participant dataset." src="Figure 15.png" class="internal-embed media-embed image-embed is-loaded"><img alt="Figure 15: Network training progress post-optimisation with a 13-participant dataset." src="figures/figure-15.png"></span></p></div><div><p><span alt="Figure 16: Network training progress post-optimisation with a single participant dataset." src="Figure 16.png" class="internal-embed media-embed image-embed is-loaded"><img alt="Figure 16: Network training progress post-optimisation with a single participant dataset." src="figures/figure-16.png"></span></p></div></div></div><div class="heading-wrapper"><h2 data-heading="Further work" class="heading" id="Further_work"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>Further work</h2><div class="heading-children"><div><p>In some incorrectly classified samples, sustained peaks were cut off by the epoching, or there were no sustained peaks at all: these examples are shown in Figure 17. </p></div><div><p><span alt="Figure 17: Examples of incorrectly classified samples compared to ERP of ideal motor imagery." src="Figure 17.png" class="internal-embed media-embed image-embed is-loaded"><img alt="Figure 17: Examples of incorrectly classified samples compared to ERP of ideal motor imagery." src="figures/figure-17.png"></span></p></div><div><p>A potential cause for this effect could be the variation in reaction time due to fatigue over the recording session. The accuracy of the network suffers as the discriminant activity caused by the movement does not make its way into the samples. Additionally, if the participant does not execute the movement that corresponds with the GUI request, then samples would be labelled inaccurately, introducing uncertainty into the network. If the participant immediately corrects an initially incorrect movement, there may be consecutive conflicting motor imageries. This could be the reason for the repeated peaks of activity in opposite lobes found in the top-left sample in Figure 17. Verification of movement could provide a conceivable solution to these issues. A method of ensuring that movement in line with the GUI took place, like a mechanical button or a motion sensor looking at each hand would provide a timestamp that the epoching could time-lock to. This would reduce the chance of motor imageries being cut off or missed. And in cases where the verified movement conflicted with the GUI request, the epoch could be rejected.</p></div><div><p>Recent research has revealed a tripolar concentric ring configuration of electrodes to have a higher spatial resolution. The method involves using a high-pass spatial filter called the surface Laplacian that sharpens blurred potential distribution from the three rings: resulting in a spatial resolution ten times more than a conventional EEG electrode <sup data-footnote-id="fnref-35-ca2cfb94fc5db613" class="footnote-ref" id="fnref-35-ca2cfb94fc5db613"><a href="#fn-35-ca2cfb94fc5db613" class="footnote-link" target="_self" rel="noopener">[35]</a></sup>. Furthermore, analysis of the gamma frequencies from EEG could provide more spatial resolution <sup data-footnote-id="fnref-14-9-ca2cfb94fc5db613" class="footnote-ref" id="fnref-14-9-ca2cfb94fc5db613"><a href="#fn-14-ca2cfb94fc5db613" class="footnote-link" target="_self" rel="noopener">[14-9]</a></sup>. However, they are more prone to noise when compared to other frequency bands due to the small amplitudes of their oscillations.</p></div><div><p>Recent developments in material science have also produced dry electrodes that offer similar signal quality with increased comfort and quicker setup. Furthermore, they do not require users to wash hair post-use <sup data-footnote-id="fnref-36-ca2cfb94fc5db613" class="footnote-ref" id="fnref-36-ca2cfb94fc5db613"><a href="#fn-36-ca2cfb94fc5db613" class="footnote-link" target="_self" rel="noopener">[36]</a></sup>. The effect on classification performance of these electrodes could be explored to allow their use.</p></div></div></div><div class="heading-wrapper"><h2 data-heading="Conclusion" class="heading" id="Conclusion"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>Conclusion</h2><div class="heading-children"><div><p>Speech, language, and communication needs can hinder emotional, social, and educational development, and are extremely prevalent in children entering primary education. High-tech augmentative and alternative communication systems that utilise speech generation can lessen the impacts of SLCN, allowing children like Amos to be understood more in the classroom. Despite the reduced need for peer training for speech-generation based AAC systems, the issues of teacher training and management of AAC devices slow down uptake. AAC systems can also serve as a distraction due to the multifunctional nature of devices today. Furthermore, AAC systems cannot be used during any sports or physical activity, leaving gaps in communication support.</p></div><div><p>Current brain-computer interfaces are not capable of replacing keyboard and touchscreen based AAC systems like the one Amos uses: their bit depth is too low for an intuitive and accessible user experience. The menu-based application interface BCI AAC systems are clunky, require memorisation for effective use and are inappropriate for young children. EEG is the most effective option for recording brain activity because of its portability, low-cost and high time resolution. Unfortunately, the poor spatial resolution of current portable EEG systems is the bottleneck for bit depth and AAC viability. Invasive, intracranial methods involving wires penetrating the skull would offer the spatial resolution needed for AAC; however, the excessive risk of tissue damage and cost of surgery makes this option unrealistic for consumer use. Research into the tripolar concentric ring configuration and gamma frequency-based classification may provide the stepping stones for BCIs to form effective AAC systems.</p></div><div><p>Neural network classifiers are fast and accurate. The LSTM recurrent neural network trained in this project was capable of classifying left- and right-hand motor imageries to the high accuracy of 91.95%. The project also finds that the patterns that the neural network exploited were not the event- related synchronisation/desynchronisation found in mu and beta rhythms. Despite this, the patterns used by the neural networks were not specific to each participant: the 13-participant classification performance was 77.01%, a value considerably greater than random chance. This result enables consumer EEG BCI systems to form much larger training datasets from motor imageries of many users, improving the classification resilience and flexibility to different conditions.</p></div></div></div><div class="heading-wrapper"><h2 data-heading="Acknowledgements" class="heading" id="Acknowledgements"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>Acknowledgements</h2><div class="heading-children"><div><p>I would like to thank Dr Zhu Tingting of the University of Oxford for her invaluable advice and support in fine-tuning parameters and analysing results, for stimulating discussion, and sparing the time to meet.</p></div><div><p>I would also like to thank Emma Smith, the mother of Amos, for her detailed interview, which provided compelling evidence integral to the argument of this project.</p></div><div><p>I would like to thank Nicholas Marsh, Dr Scott Crawford, and Magdalen College School, Oxford, for their guidance, supervision and resources.</p></div></div></div><div class="heading-wrapper"><h2 data-heading="Bibliography" class="heading" id="Bibliography"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>Bibliography</h2><div class="heading-children"><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div>
</div><div><section class="footnotes"><hr><ol>
<li data-line="-70" data-footnote-id="fn-1-ca2cfb94fc5db613" id="fn-1-ca2cfb94fc5db613">Therrien MCS, Light JC. Promoting Peer Interaction for Preschool Children With Complex Communication Needs and Autism Spectrum Disorder. Am J Speech Lang Pathol [Internet]. 2018 Feb 6 [cited 2022 May 21];27(1):207–21. Available from: <a rel="noopener" class="external-link" href="https://pubs.asha.org/doi/abs/10.1044/2017_AJSLP-" target="_blank">https://pubs.asha.org/doi/abs/10.1044/2017_AJSLP-</a> 17-0104<a href="#fnref-1-ca2cfb94fc5db613" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a><a href="#fnref-1-1-ca2cfb94fc5db613" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a></li>
<li data-line="-68" data-footnote-id="fn-2-ca2cfb94fc5db613" id="fn-2-ca2cfb94fc5db613">National Academies of Sciences, Engineering, and Medicine, Health and Medicine Division, Board on Health Care Services, Committee on the Use of Selected Assistive Products and Technologies in Eliminating or Reducing the Effects of Impairments. The Promise of Assistive Technology to Enhance Activity and Work Participation [Internet]. Flaubert JL, Spicer CM, Jette AM, editors. Washington (DC): National Academies Press (US); 2017 [cited 2022 Jan 2]. Available from: <a rel="noopener" class="external-link" href="http://www.ncbi.nlm.nih.gov/books/NBK453289/" target="_blank">http://www.ncbi.nlm.nih.gov/books/NBK453289/</a><a href="#fnref-2-ca2cfb94fc5db613" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a></li>
<li data-line="-66" data-footnote-id="fn-3-ca2cfb94fc5db613" id="fn-3-ca2cfb94fc5db613">Norbury CF, Gooch D, Wray C, Baird G, Charman T, Simonoff E, et al. The impact of nonverbal ability on prevalence and clinical presentation of language disorder: evidence from a population study. J Child Psychol Psychiatry. 2016 Nov;57(11):1247–57.<a href="#fnref-3-ca2cfb94fc5db613" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a></li>
<li data-line="-64" data-footnote-id="fn-4-ca2cfb94fc5db613" id="fn-4-ca2cfb94fc5db613">SEND code of practice: 0 to 25 years - GOV.UK [Internet]. [cited 2022 May 21]. Available from: <a rel="noopener" class="external-link" href="https://www.gov.uk/government/publications/send-code-of-practice-0-to-25" target="_blank">https://www.gov.uk/government/publications/send-code-of-practice-0-to-25</a><a href="#fnref-4-ca2cfb94fc5db613" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a></li>
<li data-line="-62" data-footnote-id="fn-5-ca2cfb94fc5db613" id="fn-5-ca2cfb94fc5db613">Verdon S, McLeod S, Wong S. Supporting culturally and linguistically diverse children with speech, language and communication needs: Overarching principles, individual approaches. J Commun Disord [Internet]. 2015 Nov 1 [cited 2022 May 21];58:74–90. Available from: <a rel="noopener" class="external-link" href="https://www.sciencedirect.com/science/article/pii/S0021992415300083" target="_blank">https://www.sciencedirect.com/science/article/pii/S0021992415300083</a><a href="#fnref-5-ca2cfb94fc5db613" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a><a href="#fnref-5-1-ca2cfb94fc5db613" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a></li>
<li data-line="-60" data-footnote-id="fn-6-ca2cfb94fc5db613" id="fn-6-ca2cfb94fc5db613">Beard A. Speech, language and communication: a public health issue across the lifecourse. Paediatr Child Health [Internet]. 2018 Mar 1 [cited 2022 May 21];28(3):126–31. Available from: <a rel="noopener" class="external-link" href="https://www.sciencedirect.com/science/article/pii/S1751722217302871" target="_blank">https://www.sciencedirect.com/science/article/pii/S1751722217302871</a><a href="#fnref-6-ca2cfb94fc5db613" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a></li>
<li data-line="-58" data-footnote-id="fn-7-ca2cfb94fc5db613" id="fn-7-ca2cfb94fc5db613">Law J, Rush R, Schoon I, Parsons S. Modeling Developmental Language Difficulties From School Entry Into Adulthood: Literacy, Mental Health, and Employment Outcomes. J Speech Lang Hear Res [Internet]. 2009 Dec 1 [cited 2022 Jan 2];52(6):1401–16. Available from: <a rel="noopener" class="external-link" href="https://pubs.asha.org/doi/10.1044/1092-" target="_blank">https://pubs.asha.org/doi/10.1044/1092-</a> 4388(2009/08-0142)<a href="#fnref-7-ca2cfb94fc5db613" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a></li>
<li data-line="-56" data-footnote-id="fn-8-ca2cfb94fc5db613" id="fn-8-ca2cfb94fc5db613">Elsahar Y, Hu S, Bouazza-Marouf K, Kerr D, Mansor A. Augmentative and Alternative Communication (AAC) Advances: A Review of Configurations for Individuals with a Speech Disability. Sensors [Internet]. 2019 Jan [cited 2022 Jan 1];19(8):1911. Available from: <a rel="noopener" class="external-link" href="https://www.mdpi.com/1424-8220/19/8/1911" target="_blank">https://www.mdpi.com/1424-8220/19/8/1911</a><a href="#fnref-8-ca2cfb94fc5db613" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a><a href="#fnref-8-1-ca2cfb94fc5db613" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a><a href="#fnref-8-2-ca2cfb94fc5db613" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a><a href="#fnref-8-3-ca2cfb94fc5db613" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a><a href="#fnref-8-4-ca2cfb94fc5db613" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a><a href="#fnref-8-5-ca2cfb94fc5db613" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a></li>
<li data-line="-54" data-footnote-id="fn-9-ca2cfb94fc5db613" id="fn-9-ca2cfb94fc5db613">Smith E. Effectiveness of augmentative and alternative communication devices and links with educational progression (Azam) [Internet]. 2022. Available from: <a rel="noopener" class="external-link" href="https://github.com/tahmidazam/waynfletestudies/" target="_blank">https://github.com/tahmidazam/waynfletestudies/</a><a href="#fnref-9-ca2cfb94fc5db613" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a></li>
<li data-line="-52" data-footnote-id="fn-10-ca2cfb94fc5db613" id="fn-10-ca2cfb94fc5db613">Kazemi M, Salehi M, Kheirollahi M. Down Syndrome: Current Status, Challenges and Future Perspectives. Int J Mol Cell Med [Internet]. 2016 [cited 2022 Jan 8];5(3):125–33. Available from: <a rel="noopener" class="external-link" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5125364/" target="_blank">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5125364/</a><a href="#fnref-10-ca2cfb94fc5db613" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a></li>
<li data-line="-50" data-footnote-id="fn-11-ca2cfb94fc5db613" id="fn-11-ca2cfb94fc5db613">Rigden A. The views of Speech and Language Therapists regarding the approaches to, and issues surrounding the diagnosis and treatment of Developmental Verbal Dyspraxia. 2018 Apr 1 [cited 2022 Jan 8]; Available from: <a rel="noopener" class="external-link" href="https://repository.cardiffmet.ac.uk/handle/10369/9911" target="_blank">https://repository.cardiffmet.ac.uk/handle/10369/9911</a><a href="#fnref-11-ca2cfb94fc5db613" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a></li>
<li data-line="-48" data-footnote-id="fn-12-ca2cfb94fc5db613" id="fn-12-ca2cfb94fc5db613">What is Makaton? - About Makaton [Internet]. [cited 2022 May 21]. Available from: <a rel="noopener" class="external-link" href="https://makaton.org/TMC/About_Makaton/What_is_Makaton.aspx" target="_blank">https://makaton.org/TMC/About_Makaton/What_is_Makaton.aspx</a><a href="#fnref-12-ca2cfb94fc5db613" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a></li>
<li data-line="-46" data-footnote-id="fn-13-ca2cfb94fc5db613" id="fn-13-ca2cfb94fc5db613">Nicolas-Alonso LF, Gomez-Gil J. Brain Computer Interfaces, a Review. Sensors [Internet]. 2012 Feb [cited 2022 Jan 2];12(2):1211–79. Available from: <a rel="noopener" class="external-link" href="https://www.mdpi.com/1424-8220/12/2/1211" target="_blank">https://www.mdpi.com/1424-8220/12/2/1211</a><a href="#fnref-13-ca2cfb94fc5db613" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a><a href="#fnref-13-1-ca2cfb94fc5db613" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a><a href="#fnref-13-2-ca2cfb94fc5db613" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a><a href="#fnref-13-3-ca2cfb94fc5db613" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a><a href="#fnref-13-4-ca2cfb94fc5db613" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a><a href="#fnref-13-5-ca2cfb94fc5db613" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a><a href="#fnref-13-6-ca2cfb94fc5db613" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a></li>
<li data-line="-44" data-footnote-id="fn-14-ca2cfb94fc5db613" id="fn-14-ca2cfb94fc5db613">Graimann B, Allison B, Pfurtscheller G. Brain–Computer Interfaces: A Gentle Introduction. In: Graimann B, Pfurtscheller G, Allison B, editors. Brain-Computer Interfaces: Revolutionizing Human-Computer Interaction [Internet]. Berlin, Heidelberg: Springer; 2010 [cited 2022 Jan 4]. p. 1–27. (The Frontiers Collection). Available from: <a rel="noopener" class="external-link" href="https://doi.org/10.1007/978-3-642-02091-9_1" target="_blank">https://doi.org/10.1007/978-3-642-02091-9_1</a><a href="#fnref-14-ca2cfb94fc5db613" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a><a href="#fnref-14-1-ca2cfb94fc5db613" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a><a href="#fnref-14-2-ca2cfb94fc5db613" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a><a href="#fnref-14-3-ca2cfb94fc5db613" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a><a href="#fnref-14-4-ca2cfb94fc5db613" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a><a href="#fnref-14-5-ca2cfb94fc5db613" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a><a href="#fnref-14-6-ca2cfb94fc5db613" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a><a href="#fnref-14-7-ca2cfb94fc5db613" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a><a href="#fnref-14-8-ca2cfb94fc5db613" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a><a href="#fnref-14-9-ca2cfb94fc5db613" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a></li>
<li data-line="-42" data-footnote-id="fn-15-ca2cfb94fc5db613" id="fn-15-ca2cfb94fc5db613">Biasiucci A, Franceschiello B, Murray MM. Electroencephalography. Curr Biol [Internet]. 2019 Feb 4 [cited 2021 Dec 8];29(3):R80–5. Available from: <a rel="noopener" class="external-link" href="https://www.sciencedirect.com/science/article/pii/S0960982218315513" target="_blank">https://www.sciencedirect.com/science/article/pii/S0960982218315513</a><a href="#fnref-15-ca2cfb94fc5db613" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a><a href="#fnref-15-1-ca2cfb94fc5db613" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a><a href="#fnref-15-2-ca2cfb94fc5db613" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a><a href="#fnref-15-3-ca2cfb94fc5db613" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a><a href="#fnref-15-4-ca2cfb94fc5db613" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a></li>
<li data-line="-40" data-footnote-id="fn-16-ca2cfb94fc5db613" id="fn-16-ca2cfb94fc5db613">Padfield N, Zabalza J, Zhao H, Masero V, Ren J. EEG-Based Brain-Computer Interfaces Using Motor- Imagery: Techniques and Challenges. Sensors [Internet]. 2019 Mar 22 [cited 2021 Dec 8];19(6):1423. Available from: <a rel="noopener" class="external-link" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6471241/" target="_blank">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6471241/</a><a href="#fnref-16-ca2cfb94fc5db613" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a><a href="#fnref-16-1-ca2cfb94fc5db613" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a></li>
<li data-line="-38" data-footnote-id="fn-17-ca2cfb94fc5db613" id="fn-17-ca2cfb94fc5db613">H. M, Samaha A, AlKamha K. Automated Classification of L/R Hand Movement EEG Signals using Advanced Feature Extraction and Machine Learning. Int J Adv Comput Sci Appl [Internet]. 2013 [cited 2021 Dec 26];4(6). Available from: <a rel="noopener" class="external-link" href="http://thesai.org/Publications/ViewPaper?Volume=4&amp;Issue=6&amp;Code=IJACSA&amp;SerialNo=28" target="_blank">http://thesai.org/Publications/ViewPaper?Volume=4&amp;Issue=6&amp;Code=IJACSA&amp;SerialNo=28</a><a href="#fnref-17-ca2cfb94fc5db613" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a><a href="#fnref-17-1-ca2cfb94fc5db613" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a><a href="#fnref-17-2-ca2cfb94fc5db613" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a><a href="#fnref-17-3-ca2cfb94fc5db613" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a></li>
<li data-line="-36" data-footnote-id="fn-18-ca2cfb94fc5db613" id="fn-18-ca2cfb94fc5db613">Uribe CF, Mathotaarachchi S, Gaudet V, Smith KC, Rosa-Neto P, Bénard F, et al. Machine Learning in Nuclear Medicine: Part 1—Introduction. J Nucl Med [Internet]. 2019 Apr 1 [cited 2021 Dec 8];60(4):451– 8. Available from: <a rel="noopener" class="external-link" href="https://jnm.snmjournals.org/content/60/4/451" target="_blank">https://jnm.snmjournals.org/content/60/4/451</a><a href="#fnref-18-ca2cfb94fc5db613" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a><a href="#fnref-18-1-ca2cfb94fc5db613" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a></li>
<li data-line="-34" data-footnote-id="fn-19-ca2cfb94fc5db613" id="fn-19-ca2cfb94fc5db613">Kaya M, Binli MK, Ozbay E, Yanar H, Mishchenko Y. A large electroencephalographic motor imagery dataset for electroencephalographic brain computer interfaces. Sci Data [Internet]. 2018 Oct 16 [cited 2021 Dec 8];5(1):180211. Available from: <a rel="noopener" class="external-link" href="https://www.nature.com/articles/sdata2018211" target="_blank">https://www.nature.com/articles/sdata2018211</a><a href="#fnref-19-ca2cfb94fc5db613" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a><a href="#fnref-19-1-ca2cfb94fc5db613" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a><a href="#fnref-19-2-ca2cfb94fc5db613" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a><a href="#fnref-19-3-ca2cfb94fc5db613" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a><a href="#fnref-19-4-ca2cfb94fc5db613" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a></li>
<li data-line="-32" data-footnote-id="fn-20-ca2cfb94fc5db613" id="fn-20-ca2cfb94fc5db613">Raghavan M, Fee D, Barkhaus PE. Chapter 1 - Generation and propagation of the action potential. In: Levin KH, Chauvel P, editors. Handbook of Clinical Neurology [Internet]. Elsevier; 2019 [cited 2022 May 21]. p. 3–22. (Clinical Neurophysiology: Basis and Technical Aspects; vol. 160). Available from: <a rel="noopener" class="external-link" href="https://www.sciencedirect.com/science/article/pii/B9780444640321000011" target="_blank">https://www.sciencedirect.com/science/article/pii/B9780444640321000011</a><a href="#fnref-20-ca2cfb94fc5db613" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a></li>
<li data-line="-30" data-footnote-id="fn-21-ca2cfb94fc5db613" id="fn-21-ca2cfb94fc5db613">Brunner C, Birbaumer N, Blankertz B, Guger C, Kübler A, Mattia D, et al. BNCI Horizon 2020: towards a roadmap for the BCI community. Brain-Comput Interfaces [Internet]. 2015 Jan 2 [cited 2022 May 21];2(1):1–10. Available from: <a rel="noopener" class="external-link" href="http://www.tandfonline.com/doi/full/10.1080/2326263X.2015.1008956" target="_blank">http://www.tandfonline.com/doi/full/10.1080/2326263X.2015.1008956</a><a href="#fnref-21-ca2cfb94fc5db613" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a><a href="#fnref-21-1-ca2cfb94fc5db613" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a><a href="#fnref-21-2-ca2cfb94fc5db613" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a><a href="#fnref-21-3-ca2cfb94fc5db613" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a><a href="#fnref-21-4-ca2cfb94fc5db613" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a></li>
<li data-line="-28" data-footnote-id="fn-22-ca2cfb94fc5db613" id="fn-22-ca2cfb94fc5db613">Nunez PL, Nunez EP of BEPL, Srinivasan R, Srinivasan AP of CSR. Electric Fields of the Brain: The Neurophysics of EEG. Oxford University Press; 2006. 629 p.<a href="#fnref-22-ca2cfb94fc5db613" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a></li>
<li data-line="-24" data-footnote-id="fn-23-ca2cfb94fc5db613" id="fn-23-ca2cfb94fc5db613">Lipton ZC, Berkowitz J, Elkan C. A Critical Review of Recurrent Neural Networks for Sequence Learning. 2015 May 29 [cited 2022 Jan 8]; Available from: <a rel="noopener" class="external-link" href="https://arxiv.org/abs/1506.00019v4" target="_blank">https://arxiv.org/abs/1506.00019v4</a><a href="#fnref-23-ca2cfb94fc5db613" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a><a href="#fnref-23-1-ca2cfb94fc5db613" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a><a href="#fnref-23-2-ca2cfb94fc5db613" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a></li>
<li data-line="-22" data-footnote-id="fn-24-ca2cfb94fc5db613" id="fn-24-ca2cfb94fc5db613">Sherstinsky A. Fundamentals of Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM) network. Phys Nonlinear Phenom [Internet]. 2020 Mar 1 [cited 2022 Jan 8];404:132306. Available from: <a rel="noopener" class="external-link" href="https://www.sciencedirect.com/science/article/pii/S0167278919305974" target="_blank">https://www.sciencedirect.com/science/article/pii/S0167278919305974</a><a href="#fnref-24-ca2cfb94fc5db613" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a></li>
<li data-line="-26" data-footnote-id="fn-25-ca2cfb94fc5db613" id="fn-25-ca2cfb94fc5db613">Staudemeyer RC, Morris ER. Understanding LSTM -- a tutorial into Long Short-Term Memory Recurrent Neural Networks [Internet]. arXiv; 2019 Sep [cited 2022 May 22]. Report No.: arXiv:1909.09586. Available from: <a rel="noopener" class="external-link" href="http://arxiv.org/abs/1909.09586" target="_blank">http://arxiv.org/abs/1909.09586</a><a href="#fnref-25-ca2cfb94fc5db613" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a></li>
<li data-line="-20" data-footnote-id="fn-26-ca2cfb94fc5db613" id="fn-26-ca2cfb94fc5db613">Kilteni K, Andersson BJ, Houborg C, Ehrsson HH. Motor imagery involves predicting the sensory consequences of the imagined movement. Nat Commun [Internet]. 2018 Apr 24 [cited 2021 Dec 8];9(1):1617. Available from: <a rel="noopener" class="external-link" href="https://www.nature.com/articles/s41467-018-03989-0" target="_blank">https://www.nature.com/articles/s41467-018-03989-0</a><a href="#fnref-26-ca2cfb94fc5db613" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a></li>
<li data-line="-18" data-footnote-id="fn-27-ca2cfb94fc5db613" id="fn-27-ca2cfb94fc5db613">MATLAB - MathWorks [Internet]. [cited 2022 Jan 9]. Available from: <a rel="noopener" class="external-link" href="https://uk.mathworks.com/products/matlab.html" target="_blank">https://uk.mathworks.com/products/matlab.html</a><a href="#fnref-27-ca2cfb94fc5db613" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a></li>
<li data-line="-16" data-footnote-id="fn-28-ca2cfb94fc5db613" id="fn-28-ca2cfb94fc5db613">EEGLAB [Internet]. [cited 2022 Jan 9]. Available from: <a rel="noopener" class="external-link" href="https://sccn.ucsd.edu/eeglab/index.php" target="_blank">https://sccn.ucsd.edu/eeglab/index.php</a><a href="#fnref-28-ca2cfb94fc5db613" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a></li>
<li data-line="-14" data-footnote-id="fn-29-ca2cfb94fc5db613" id="fn-29-ca2cfb94fc5db613">c. Channel Locations [Internet]. EEGLAB Wiki. [cited 2022 May 16]. Available from: <a rel="noopener" class="external-link" href="https://eeglab.org/tutorials/04_Import/Channel_Locations.html" target="_blank">https://eeglab.org/tutorials/04_Import/Channel_Locations.html</a><a href="#fnref-29-ca2cfb94fc5db613" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a></li>
<li data-line="-12" data-footnote-id="fn-30-ca2cfb94fc5db613" id="fn-30-ca2cfb94fc5db613">c. Automated rejection [Internet]. EEGLAB Wiki. [cited 2022 May 15]. Available from: <a rel="noopener" class="external-link" href="https://eeglab.org/tutorials/06_RejectArtifacts/cleanrawdata.html" target="_blank">https://eeglab.org/tutorials/06_RejectArtifacts/cleanrawdata.html</a><a href="#fnref-30-ca2cfb94fc5db613" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a></li>
<li data-line="-10" data-footnote-id="fn-31-ca2cfb94fc5db613" id="fn-31-ca2cfb94fc5db613">EEGLAB [Internet]. [cited 2022 Jan 8]. Available from: <a rel="noopener" class="external-link" href="https://sccn.ucsd.edu/eeglab/index.php" target="_blank">https://sccn.ucsd.edu/eeglab/index.php</a><a href="#fnref-31-ca2cfb94fc5db613" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a></li>
<li data-line="-8" data-footnote-id="fn-32-ca2cfb94fc5db613" id="fn-32-ca2cfb94fc5db613">Onton J, Westerfield M, Townsend J, Makeig S. Imaging human EEG dynamics using independent component analysis. Neurosci Biobehav Rev [Internet]. 2006 Jan 1 [cited 2021 Dec 29];30(6):808–22. Available from: <a rel="noopener" class="external-link" href="https://www.sciencedirect.com/science/article/pii/S0149763406000509" target="_blank">https://www.sciencedirect.com/science/article/pii/S0149763406000509</a><a href="#fnref-32-ca2cfb94fc5db613" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a></li>
<li data-line="-6" data-footnote-id="fn-33-ca2cfb94fc5db613" id="fn-33-ca2cfb94fc5db613">Abbasi-Kesbi R, Memarzadeh-Tehran H, Deen MJ. Technique to estimate human reaction time based on visual perception. Healthc Technol Lett [Internet]. 2017 [cited 2022 May 15];4(2):73–7. Available from: <a rel="noopener" class="external-link" href="https://onlinelibrary.wiley.com/doi/abs/10.1049/htl.2016.0106" target="_blank">https://onlinelibrary.wiley.com/doi/abs/10.1049/htl.2016.0106</a><a href="#fnref-33-ca2cfb94fc5db613" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a></li>
<li data-line="-4" data-footnote-id="fn-34-ca2cfb94fc5db613" id="fn-34-ca2cfb94fc5db613">Banerjee K, C VP, Gupta RR, Vyas K, H A, Mishra B. Exploring Alternatives to Softmax Function [Internet]. arXiv; 2020 Nov [cited 2022 May 16]. Report No.: arXiv:2011.11538. Available from: <a rel="noopener" class="external-link" href="http://arxiv.org/abs/2011.11538" target="_blank">http://arxiv.org/abs/2011.11538</a><a href="#fnref-34-ca2cfb94fc5db613" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a></li>
<li data-line="-2" data-footnote-id="fn-35-ca2cfb94fc5db613" id="fn-35-ca2cfb94fc5db613">Liu X, Makeyev O, Besio W. Improved Spatial Resolution of Electroencephalogram Using Tripolar Concentric Ring Electrode Sensors. J Sens [Internet]. 2020 Jun 8 [cited 2022 Mar 3];2020:e6269394. Available from: <a rel="noopener" class="external-link" href="https://www.hindawi.com/journals/js/2020/6269394/" target="_blank">https://www.hindawi.com/journals/js/2020/6269394/</a><a href="#fnref-35-ca2cfb94fc5db613" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a></li>
<li data-line="0" data-footnote-id="fn-36-ca2cfb94fc5db613" id="fn-36-ca2cfb94fc5db613">Umcu M van S, Umcu GK, Blefari M, Brunner C, Blankertz B, Höhne J, et al. Title: Contribution to Roadmap [Internet]. 2014 [cited 2021 Dec 31]. Available from: <a rel="noopener" class="external-link" href="https://www.semanticscholar.org/paper/Title%3A-Contribution-to-Roadmap-Umcu-" target="_blank">https://www.semanticscholar.org/paper/Title%3A-Contribution-to-Roadmap-Umcu-</a> Umcu/5cb411de3db4941d5c7ecfc19de8af9243fb63d5<a href="#fnref-36-ca2cfb94fc5db613" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a></li>
</ol></section></div><div class="mod-footer"></div></div></div></div></div></div><div class="sidebar-right sidebar"><div class="sidebar-handle"></div><div class="sidebar-topbar"><div class="topbar-content"></div><div class="clickable-icon sidebar-collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="100%" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="3" stroke-linecap="round" stroke-linejoin="round" class="svg-icon"><path d="M21 3H3C1.89543 3 1 3.89543 1 5V19C1 20.1046 1.89543 21 3 21H21C22.1046 21 23 20.1046 23 19V5C23 3.89543 22.1046 3 21 3Z"></path><path d="M10 4V20"></path><path d="M4 7H7"></path><path d="M4 10H7"></path><path d="M4 13H7"></path></svg></div></div><div class="sidebar-content"></div><script defer="">let rs = document.querySelector(".sidebar-right"); rs.classList.add("is-collapsed"); if (window.innerWidth > 768) rs.classList.remove("is-collapsed"); rs.style.setProperty("--sidebar-width", localStorage.getItem("sidebar-right-width"));</script></div></div></body></html>